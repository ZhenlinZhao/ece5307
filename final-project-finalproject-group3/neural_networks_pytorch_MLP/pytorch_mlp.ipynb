{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls_IE1OGdPQi"
      },
      "source": [
        "# training demo for pytorch models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fZpoMKhodPQn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6wp9Q96edPQo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# load training data\n",
        "Xtr_loadpath = 'Xtr.csv'\n",
        "Xts_loadpath = 'Xts.csv'\n",
        "ytr_loadpath = 'ytr.csv'\n",
        "\n",
        "Xtr = np.loadtxt(Xtr_loadpath, delimiter=\",\")\n",
        "Xts = np.loadtxt(Xts_loadpath, delimiter=\",\")\n",
        "ytr = np.loadtxt(ytr_loadpath, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "zC5yHPaLdPQp"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "\n",
        "# standardize the training data\n",
        "Xtr_standardized = scaler.transform(Xtr) # revise this line as needed\n",
        "Xts_standardized = scaler.transform(Xts) # revise this line as needed\n",
        "ytr_standardized = ytr # revise this line as needed\n",
        "\n",
        "\n",
        "# save the standardized training data\n",
        "Xtr_savepath = 'Xtr_pytorch.csv'\n",
        "Xts_savepath = 'Xts_pytorch.csv'\n",
        "ytr_savepath = 'ytr_pytorch.csv'\n",
        "yts_hat_savepath = 'yts_hat_pytorch.csv'\n",
        "\n",
        "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
        "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
        "np.savetxt(ytr_savepath, ytr_standardized, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8uE2dyfV6if4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MV38_a-V6o6K"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(Xtr_standardized, ytr_standardized, test_size=0.2, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ze3PtTWbYH0C"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "for i in range(len(X_train)):\n",
        "   train_data.append([X_train[i], y_train[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Edm_K6S5Ke7e"
      },
      "outputs": [],
      "source": [
        "all_data = []\n",
        "for i in range(len(Xtr_standardized)):\n",
        "   all_data.append([Xtr_standardized[i], ytr_standardized[i]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qOmFVrCvahVa"
      },
      "source": [
        "# Multilayer Perceptron (MLP)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ejfNGn_oF_RH"
      },
      "source": [
        "First, we try MLP with 5 layers and 128 nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "madsVaqHdPQp"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "d_in = Xtr.shape[1]\n",
        "d_out = 1\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,n_layer,n_hidden_nodes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in,n_hidden_nodes))\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Linear(n_hidden_nodes,n_hidden_nodes))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes,d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "model = MLP(5, 128)   \n",
        "\n",
        "# Usually, we would train the model at this point. \n",
        "# But this is only a demo, so we'll use the randomly initialized weights."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fTjqlx7rGFDk"
      },
      "source": [
        "We use 10 epochs, adam, with learning rate = 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hvhEijU0XKbX"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "woEs6EXPXT25"
      },
      "outputs": [],
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bt9zVUPcYm3s"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YxskY18XDPX",
        "outputId": "1bb56fad-20c7-4ccd-8488-2eafb205dc4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5054535661407717 0.8259737377393941\n",
            "Starting epoch 2\n",
            "0.46425104191647665 0.8189104221613144\n",
            "Starting epoch 3\n",
            "0.46367769256881297 0.8217493234041819\n",
            "Starting epoch 4\n",
            "0.46401785087935876 0.8213991742945304\n",
            "Starting epoch 5\n",
            "0.46331089944267867 0.8190504818051749\n",
            "Starting epoch 6\n",
            "0.46380043747354555 0.8205114116288291\n",
            "Starting epoch 7\n",
            "0.4640483409225321 0.8213927100032753\n",
            "Starting epoch 8\n",
            "0.463547465964918 0.8217353174397959\n",
            "Starting epoch 9\n",
            "0.46397043197904736 0.8156599610418713\n",
            "Starting epoch 10\n",
            "0.46407858234473975 0.8187369636793023\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs.float())[:, 0]\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
        "\n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ClO-HOQAGcY_"
      },
      "source": [
        "Results are't very good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mV8lfJq6arUi"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "d_in = Xtr.shape[1]\n",
        "d_out = 1\n",
        "\n",
        "class MLPA(nn.Module):\n",
        "    def __init__(self,n_layer,n_hidden_nodes):\n",
        "        super(MLPA, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in,n_hidden_nodes))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Linear(n_hidden_nodes,n_hidden_nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes,d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "model = MLPA(5, 128)   \n",
        "\n",
        "# Usually, we would train the model at this point. \n",
        "# But this is only a demo, so we'll use the randomly initialized weights."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NZchmCj6GfCV"
      },
      "source": [
        "We add a ReLU. It's the best in activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kS_e4gq0arUi"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwsMoPCgarUj",
        "outputId": "cdfdcf4a-e0f1-474f-9737-8991c79b1b37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5513488252417738 0.8528975108169141\n",
            "Starting epoch 2\n",
            "0.4229204962634984 0.8711731395769766\n",
            "Starting epoch 3\n",
            "0.4030552740888335 0.8812251124786679\n",
            "Starting epoch 4\n",
            "0.3880549971514155 0.8870160400613677\n",
            "Starting epoch 5\n",
            "0.37623526695808795 0.8884306424643602\n",
            "Starting epoch 6\n",
            "0.3711684930192926 0.8938218613711193\n",
            "Starting epoch 7\n",
            "0.3608013035322753 0.8950166778714381\n",
            "Starting epoch 8\n",
            "0.3540631747767757 0.8960541966178828\n",
            "Starting epoch 9\n",
            "0.34993483654660035 0.8966683042871179\n",
            "Starting epoch 10\n",
            "0.34325054836979746 0.8951933684990778\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs.float())[:, 0]\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
        "\n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pW8I-UNKGj5Q"
      },
      "source": [
        "Results are better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ozG6I21Da-S8"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "d_in = Xtr.shape[1]\n",
        "d_out = 1\n",
        "\n",
        "class MLPB(nn.Module):\n",
        "    def __init__(self,n_layer,n_hidden_nodes):\n",
        "        super(MLPB, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in,n_hidden_nodes))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Dropout(p=0.2))\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Linear(n_hidden_nodes,n_hidden_nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(p=0.2))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes,d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "model = MLPB(5, 128)   \n",
        "\n",
        "# Usually, we would train the model at this point. \n",
        "# But this is only a demo, so we'll use the randomly initialized weights."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CgvW8zlyGlLx"
      },
      "source": [
        "We add a dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wF1FekY-a-S-"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oBg2WrEa-S-",
        "outputId": "80237899-6d21-4c9e-896e-dc873312b84d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.6111168710871134 0.8239579562496768\n",
            "Starting epoch 2\n",
            "0.45536657956961746 0.8402221130475255\n",
            "Starting epoch 3\n",
            "0.4330727564946297 0.8549434589991554\n",
            "Starting epoch 4\n",
            "0.4261213980894089 0.8558118287910913\n",
            "Starting epoch 5\n",
            "0.42307524787538775 0.8585386823188706\n",
            "Starting epoch 6\n",
            "0.41936601717788125 0.8627480133078208\n",
            "Starting epoch 7\n",
            "0.41669071418285547 0.8581745272448329\n",
            "Starting epoch 8\n",
            "0.40846976497855747 0.867884970091879\n",
            "Starting epoch 9\n",
            "0.40812468569109034 0.857869628173967\n",
            "Starting epoch 10\n",
            "0.40208128460603787 0.8672396183482443\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs.float())[:, 0]\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
        "\n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bvveiDqRGokc"
      },
      "source": [
        "Results are bad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TSW-oDtSbT0W"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "d_in = Xtr.shape[1]\n",
        "d_out = 1\n",
        "\n",
        "class MLPC(nn.Module):\n",
        "    def __init__(self,n_layer,n_hidden_nodes):\n",
        "        super(MLPC, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in,n_hidden_nodes))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Linear(n_hidden_nodes,n_hidden_nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(p=0.1))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes,d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "model = MLPC(5, 128)   \n",
        "\n",
        "# Usually, we would train the model at this point. \n",
        "# But this is only a demo, so we'll use the randomly initialized weights."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Q2-wvHGqhL"
      },
      "source": [
        "Then, we change the coefficients of dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8qsxBzjbbT0X"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ixl_ohBJbT0X"
      },
      "outputs": [],
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0GRJqe9jbT0X"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne3bXbhNbT0X",
        "outputId": "5e02a31a-5406-41ca-ed75-4e99c3a536ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5667247169745242 0.8505261933081658\n",
            "Starting epoch 2\n",
            "0.42691413896939956 0.8607106841805865\n",
            "Starting epoch 3\n",
            "0.41464166230920324 0.8706613831859475\n",
            "Starting epoch 4\n",
            "0.40516830843996243 0.8754072503490716\n",
            "Starting epoch 5\n",
            "0.3954928434995254 0.8855971281308718\n",
            "Starting epoch 6\n",
            "0.38525390523708547 0.8852372825843376\n",
            "Starting epoch 7\n",
            "0.37784176310090684 0.8916186154350036\n",
            "Starting epoch 8\n",
            "0.3680772134413367 0.8917220440950855\n",
            "Starting epoch 9\n",
            "0.36410339514067797 0.8945609453379533\n",
            "Starting epoch 10\n",
            "0.35679744078121073 0.893918825739946\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "      inputs, targets = data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs.float())[:, 0]\n",
        "      loss = loss_function(outputs, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
        "\n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8vQj82IXGtEs"
      },
      "source": [
        "It looks better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qIUgH5m9bmew"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "d_in = Xtr.shape[1]\n",
        "d_out = 1\n",
        "\n",
        "class MLPA(nn.Module):\n",
        "    def __init__(self,n_layer,n_hidden_nodes):\n",
        "        super(MLPA, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in,n_hidden_nodes))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Linear(n_hidden_nodes,n_hidden_nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes,d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "# Usually, we would train the model at this point. \n",
        "# But this is only a demo, so we'll use the randomly initialized weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ou2nnfpZbmew"
      },
      "outputs": [],
      "source": [
        "n_epochs = 10"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "94eELln1GvuO"
      },
      "source": [
        "We test different layers and nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZKeqPSVbyal",
        "outputId": "c1167861-3d60-4a13-cce0-e41e0eb07156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 32\n",
            "Starting epoch 1\n",
            "0.6717895442913286 0.7858035544982849\n",
            "Starting epoch 2\n",
            "0.5845040188594721 0.8361334488286705\n",
            "Starting epoch 3\n",
            "0.44976805001447795 0.8501555739428729\n",
            "Starting epoch 4\n",
            "0.4223825761491985 0.8584234024581545\n",
            "Starting epoch 5\n",
            "0.41304674379159884 0.8625249952595199\n",
            "Starting epoch 6\n",
            "0.4076234583925347 0.8653919084311595\n",
            "Starting epoch 7\n",
            "0.40414094446145576 0.8679399165675474\n",
            "Starting epoch 8\n",
            "0.40166006070756327 0.8691799831066521\n",
            "Starting epoch 9\n",
            "0.39929758630491624 0.8707583475547742\n",
            "Starting epoch 10\n",
            "0.396829183841182 0.8726437658375137\n",
            "5 64\n",
            "Starting epoch 1\n",
            "0.628025432755705 0.8251560048956234\n",
            "Starting epoch 2\n",
            "0.4458894456909056 0.8521972125976107\n",
            "Starting epoch 3\n",
            "0.4260092889289511 0.8626402751202358\n",
            "Starting epoch 4\n",
            "0.41435422991377724 0.8709285738911586\n",
            "Starting epoch 5\n",
            "0.4042082157352367 0.878163193187499\n",
            "Starting epoch 6\n",
            "0.3961412164425991 0.8818359880022755\n",
            "Starting epoch 7\n",
            "0.38940128310064787 0.8862974263501748\n",
            "Starting epoch 8\n",
            "0.3839844628679459 0.8888852976159695\n",
            "Starting epoch 9\n",
            "0.37867590169427673 0.8915011808105359\n",
            "Starting epoch 10\n",
            "0.3737519187038772 0.8917360500594715\n",
            "5 128\n",
            "Starting epoch 1\n",
            "0.552901933349538 0.8537820413369878\n",
            "Starting epoch 2\n",
            "0.4233084098276772 0.8683945717191566\n",
            "Starting epoch 3\n",
            "0.40648128692146457 0.8796855337780766\n",
            "Starting epoch 4\n",
            "0.3931677603285527 0.8859461998586475\n",
            "Starting epoch 5\n",
            "0.38275085011002397 0.8879587492027374\n",
            "Starting epoch 6\n",
            "0.3731141743696099 0.8896696316215891\n",
            "Starting epoch 7\n",
            "0.3661478934037467 0.8938768078467878\n",
            "Starting epoch 8\n",
            "0.3586648745140346 0.8949875885607903\n",
            "Starting epoch 9\n",
            "0.35257685396531635 0.8953248090879316\n",
            "Starting epoch 10\n",
            "0.3493321878279488 0.8946762251986693\n",
            "5 256\n",
            "Starting epoch 1\n",
            "0.4932978191506643 0.8645267707848512\n",
            "Starting epoch 2\n",
            "0.4067788334094048 0.8857167175190912\n",
            "Starting epoch 3\n",
            "0.3868184478571556 0.8906274241092207\n",
            "Starting epoch 4\n",
            "0.36203028818024025 0.8971498939856234\n",
            "Starting epoch 5\n",
            "0.35369937859365247 0.8988219906569442\n",
            "Starting epoch 6\n",
            "0.344013785310644 0.9023224043715846\n",
            "Starting epoch 7\n",
            "0.33622550277841323 0.8988112168381858\n",
            "Starting epoch 8\n",
            "0.3285702937352706 0.9011965403113202\n",
            "Starting epoch 9\n",
            "0.32638997077560855 0.901668433572943\n",
            "Starting epoch 10\n",
            "0.31975533734516676 0.9044556204857699\n",
            "10 32\n",
            "Starting epoch 1\n",
            "0.6661011959593743 0.8200319766940752\n",
            "Starting epoch 2\n",
            "0.5904742535352707 0.8362002465049732\n",
            "Starting epoch 3\n",
            "0.5019153091916524 0.846144481219079\n",
            "Starting epoch 4\n",
            "0.4496842648232123 0.8537626484632226\n",
            "Starting epoch 5\n",
            "0.4318360791612067 0.8581098843322819\n",
            "Starting epoch 6\n",
            "0.4242941222839727 0.8619938459947252\n",
            "Starting epoch 7\n",
            "0.4180924270516771 0.8642865146265364\n",
            "Starting epoch 8\n",
            "0.4147161602345368 0.8670392253193361\n",
            "Starting epoch 9\n",
            "0.41127712626894575 0.8686628398062437\n",
            "Starting epoch 10\n",
            "0.40752092307665044 0.8704006567719914\n",
            "10 64\n",
            "Starting epoch 1\n",
            "0.6503798121940345 0.8231068245677543\n",
            "Starting epoch 2\n",
            "0.4759738517320948 0.8529503025288308\n",
            "Starting epoch 3\n",
            "0.43232884228094187 0.8620606436710279\n",
            "Starting epoch 4\n",
            "0.4170163614917892 0.8681090655220561\n",
            "Starting epoch 5\n",
            "0.406294790264692 0.8736500405095585\n",
            "Starting epoch 6\n",
            "0.39980294928154275 0.8772862043405562\n",
            "Starting epoch 7\n",
            "0.39522063512666933 0.8797997362569167\n",
            "Starting epoch 8\n",
            "0.39118906026891115 0.8809299098446847\n",
            "Starting epoch 9\n",
            "0.38556055924848676 0.8833260071365776\n",
            "Starting epoch 10\n",
            "0.3810252071002598 0.8850358121735532\n",
            "10 128\n",
            "Starting epoch 1\n",
            "0.570542782981589 0.8494541983416939\n",
            "Starting epoch 2\n",
            "0.43501116658211253 0.8664929927082796\n",
            "Starting epoch 3\n",
            "0.4091616397874081 0.8764092154936133\n",
            "Starting epoch 4\n",
            "0.39636180553586736 0.8852566754581028\n",
            "Starting epoch 5\n",
            "0.3829055549409504 0.8874943976142455\n",
            "Starting epoch 6\n",
            "0.3752270025535788 0.8885868628363587\n",
            "Starting epoch 7\n",
            "0.3636303396436934 0.8888680595059557\n",
            "Starting epoch 8\n",
            "0.354032157457242 0.8900585664787712\n",
            "Starting epoch 9\n",
            "0.3486925179088498 0.8932702418506835\n",
            "Starting epoch 10\n",
            "0.3426781472720877 0.8914623950630054\n",
            "10 256\n",
            "Starting epoch 1\n",
            "0.5147167747950443 0.8624775904569822\n",
            "Starting epoch 2\n",
            "0.4106836829527989 0.878827937804899\n",
            "Starting epoch 3\n",
            "0.3856474151125541 0.8916617107100379\n",
            "Starting epoch 4\n",
            "0.3728726957614198 0.89316465842685\n",
            "Starting epoch 5\n",
            "0.35434465916323565 0.8957191308544933\n",
            "Starting epoch 6\n",
            "0.3500272333526866 0.8974375549464758\n",
            "Starting epoch 7\n",
            "0.3391136985519363 0.8998261105652376\n",
            "Starting epoch 8\n",
            "0.33153041736439276 0.9018041836893003\n",
            "Starting epoch 9\n",
            "0.3255490532388474 0.8988866335694955\n",
            "Starting epoch 10\n",
            "0.32117959813791835 0.8991818362034787\n",
            "15 32\n",
            "Starting epoch 1\n",
            "0.6809364386009984 0.8100898967437211\n",
            "Starting epoch 2\n",
            "0.5936749090491794 0.8334044405371395\n",
            "Starting epoch 3\n",
            "0.5152635777079267 0.8387353260588509\n",
            "Starting epoch 4\n",
            "0.49802216847019737 0.8423036148316698\n",
            "Starting epoch 5\n",
            "0.45834424726385625 0.8473716191756737\n",
            "Starting epoch 6\n",
            "0.4383105360637419 0.8539177914533451\n",
            "Starting epoch 7\n",
            "0.425308440424611 0.86077209494751\n",
            "Starting epoch 8\n",
            "0.4160498137063551 0.8660351054110428\n",
            "Starting epoch 9\n",
            "0.40933507932740393 0.8707206391891192\n",
            "Starting epoch 10\n",
            "0.4042163360097948 0.8745625829584045\n",
            "15 64\n",
            "Starting epoch 1\n",
            "0.6573255492807366 0.82636698212408\n",
            "Starting epoch 2\n",
            "0.523382533421478 0.8450767957801107\n",
            "Starting epoch 3\n",
            "0.44947980557743905 0.8591527899881057\n",
            "Starting epoch 4\n",
            "0.4186465764911189 0.8708466928685941\n",
            "Starting epoch 5\n",
            "0.40134698682078135 0.8798094326937994\n",
            "Starting epoch 6\n",
            "0.3893817004372706 0.8828088638361691\n",
            "Starting epoch 7\n",
            "0.3819072745867094 0.8862575632207684\n",
            "Starting epoch 8\n",
            "0.37449561227524175 0.8878542431607799\n",
            "Starting epoch 9\n",
            "0.3675971434600803 0.887882255089552\n",
            "Starting epoch 10\n",
            "0.3644096760586003 0.8891611073761871\n",
            "15 128\n",
            "Starting epoch 1\n",
            "0.6116078788128216 0.8425654186275017\n",
            "Starting epoch 2\n",
            "0.46187448923329794 0.8612159762803606\n",
            "Starting epoch 3\n",
            "0.41100038700399416 0.8748944165761666\n",
            "Starting epoch 4\n",
            "0.3930174355127183 0.8850638241023254\n",
            "Starting epoch 5\n",
            "0.38041625756540626 0.8860668666287428\n",
            "Starting epoch 6\n",
            "0.36729268354611455 0.8832818344796676\n",
            "Starting epoch 7\n",
            "0.3602007548951975 0.8858524676354483\n",
            "Starting epoch 8\n",
            "0.3548362434291832 0.8932465394494148\n",
            "Starting epoch 9\n",
            "0.34957110893758386 0.8924331161331471\n",
            "Starting epoch 10\n",
            "0.34181396186193375 0.8950112909620589\n",
            "15 256\n",
            "Starting epoch 1\n",
            "0.5560778376976814 0.862479745220734\n",
            "Starting epoch 2\n",
            "0.413891810210745 0.8836492216993328\n",
            "Starting epoch 3\n",
            "0.38694789503589755 0.8929265570322871\n",
            "Starting epoch 4\n",
            "0.3682085927193792 0.8940782782575718\n",
            "Starting epoch 5\n",
            "0.35794465682165494 0.8945329334091812\n",
            "Starting epoch 6\n",
            "0.3444385750329938 0.8983350140490596\n",
            "Starting epoch 7\n",
            "0.34244363393913957 0.9020250469738498\n",
            "Starting epoch 8\n",
            "0.3329225776132025 0.901726612194239\n",
            "Starting epoch 9\n",
            "0.3267316230724019 0.9030636431021701\n",
            "Starting epoch 10\n",
            "0.3193735796952194 0.9027522797400493\n"
          ]
        }
      ],
      "source": [
        "n_layer_list = [5, 10, 15]\n",
        "n_hidden_nodes_list = [32, 64, 128, 256]\n",
        "for n_layer in n_layer_list:\n",
        "    for n_hidden_nodes in n_hidden_nodes_list:\n",
        "        print(n_layer, n_hidden_nodes)\n",
        "        model = MLPA(n_layer, n_hidden_nodes)   \n",
        "        loss_function = nn.BCEWithLogitsLoss()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "        for epoch in range(n_epochs): \n",
        "            print(f'Starting epoch {epoch+1}')\n",
        "            current_loss = 0.0\n",
        "            for i, data in enumerate(trainloader):\n",
        "                inputs, targets = data\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs.float())[:, 0]\n",
        "                loss = loss_function(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                current_loss += loss.item()\n",
        "            with torch.no_grad():\n",
        "                predict = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
        "\n",
        "            auc = roc_auc_score(y_val,predict)\n",
        "            print(current_loss / len(trainloader), auc)\n",
        "            current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DcqLCO8sG6I1"
      },
      "source": [
        "Then, we try the seemingly the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaeJ9JKEfJQ1",
        "outputId": "6f24c2c4-3a70-42fa-eb81-80138310d9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5239376040049392 0.8700160314423128\n",
            "Starting epoch 2\n",
            "0.4066072223409655 0.8826871196841979\n",
            "Starting epoch 3\n",
            "0.388207365955073 0.8898743341780007\n",
            "Starting epoch 4\n",
            "0.3755391801627264 0.8937065815104033\n",
            "Starting epoch 5\n",
            "0.3591369476954469 0.8969753581217356\n",
            "Starting epoch 6\n",
            "0.34837869137144467 0.9009638258261363\n",
            "Starting epoch 7\n",
            "0.33903869611942944 0.9002646049887091\n",
            "Starting epoch 8\n",
            "0.33266079483046324 0.9020724517763873\n",
            "Starting epoch 9\n",
            "0.32659682352171177 0.9025955206771129\n",
            "Starting epoch 10\n",
            "0.32023414751703866 0.9027738273775663\n",
            "Starting epoch 11\n",
            "0.3187962272938094 0.8984589129647824\n",
            "Starting epoch 12\n",
            "0.3126279931137938 0.9040666856285877\n",
            "Starting epoch 13\n",
            "0.3055676009959126 0.8978943648618366\n",
            "Starting epoch 14\n",
            "0.3106776260108044 0.905454353484684\n",
            "Starting epoch 15\n",
            "0.29934854478002904 0.9048316267604419\n",
            "Starting epoch 16\n",
            "0.29570852100558376 0.9035991018944682\n",
            "Starting epoch 17\n",
            "0.2938262941625306 0.908682189584734\n",
            "Starting epoch 18\n",
            "0.2886220541319852 0.9063464256778886\n",
            "Starting epoch 19\n",
            "0.2844389586302057 0.9041485666511524\n",
            "Starting epoch 20\n",
            "0.2804026609794935 0.9029117322576753\n",
            "Starting epoch 21\n",
            "0.2797240800211605 0.9033836255192981\n",
            "Starting epoch 22\n",
            "0.27280208258768424 0.9020541362844978\n",
            "Starting epoch 23\n",
            "0.2700655139753305 0.9040268224991811\n",
            "Starting epoch 24\n",
            "0.2627037227300406 0.8947451776387237\n",
            "Starting epoch 25\n",
            "0.2656887742881895 0.9016091775697712\n",
            "Starting epoch 26\n",
            "0.26035178107657886 0.8970195307786455\n",
            "Starting epoch 27\n",
            "0.2608155059502441 0.9005425695126789\n",
            "Starting epoch 28\n",
            "0.24830282018167169 0.8971972987881609\n",
            "Starting epoch 29\n",
            "0.24667766498702232 0.898267138990881\n",
            "Starting epoch 30\n",
            "0.2408067042070973 0.8971391201668649\n",
            "Starting epoch 31\n",
            "0.23341548124952813 0.8901522987019703\n",
            "Starting epoch 32\n",
            "0.2311572301641983 0.8878574753064074\n",
            "Starting epoch 33\n",
            "0.2265310304151502 0.8776675975246073\n",
            "Starting epoch 34\n",
            "0.22278136044570135 0.8839142576407923\n",
            "Starting epoch 35\n",
            "0.2178406533985344 0.8804601713468136\n",
            "Starting epoch 36\n",
            "0.21289847833874812 0.888222707762321\n",
            "Starting epoch 37\n",
            "0.21026569661053612 0.8934738670252194\n",
            "Starting epoch 38\n",
            "0.20305415544632768 0.8795573253348503\n",
            "Starting epoch 39\n",
            "0.19942172655312146 0.8807424453982865\n",
            "Starting epoch 40\n",
            "0.20130792264529948 0.8810613504335384\n",
            "Starting epoch 41\n",
            "0.18416280053292844 0.8693792987536847\n",
            "Starting epoch 42\n",
            "0.18809518216978113 0.8858740152729654\n",
            "Starting epoch 43\n",
            "0.1780908875450778 0.8721449380289945\n",
            "Starting epoch 44\n",
            "0.1707294098571568 0.8828735067487201\n",
            "Starting epoch 45\n",
            "0.17240251957176922 0.8786350864491217\n",
            "Starting epoch 46\n",
            "0.1652748456170077 0.877121364913551\n",
            "Starting epoch 47\n",
            "0.16371263279459652 0.8795174622054438\n",
            "Starting epoch 48\n",
            "0.15885266775881543 0.8574882349899157\n",
            "Starting epoch 49\n",
            "0.15586512095534166 0.8749213511230629\n",
            "Starting epoch 50\n",
            "0.14501729298618862 0.8877885228663528\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "model = MLPA(10, 256)   \n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
        "\n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz8R1qbdg0so",
        "outputId": "37118aa9-32cb-44dd-d205-569c84916e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5523627452963265 0.8511596938511662\n",
            "Starting epoch 2\n",
            "0.4199855882064271 0.8737416179690058\n",
            "Starting epoch 3\n",
            "0.4014262285303612 0.880897588388409\n",
            "Starting epoch 4\n",
            "0.3881188465238383 0.8850735205392082\n",
            "Starting epoch 5\n",
            "0.375094325030704 0.8898667925048698\n",
            "Starting epoch 6\n",
            "0.36566543429456716 0.8917188119494579\n",
            "Starting epoch 7\n",
            "0.359708913596397 0.8933952181482823\n",
            "Starting epoch 8\n",
            "0.3532517278635955 0.8958371041698987\n",
            "Starting epoch 9\n",
            "0.34749436089611113 0.8952881781041528\n",
            "Starting epoch 10\n",
            "0.34520803648646214 0.8968309889503715\n",
            "Starting epoch 11\n",
            "0.33914491534175994 0.8986776214855803\n",
            "Starting epoch 12\n",
            "0.3360897203246972 0.8955661426281223\n",
            "Starting epoch 13\n",
            "0.33570560769202984 0.8954939580424401\n",
            "Starting epoch 14\n",
            "0.32898205427114013 0.8982423592077364\n",
            "Starting epoch 15\n",
            "0.3257086323650081 0.9004348313250936\n",
            "Starting epoch 16\n",
            "0.3228063206130579 0.9003895812863076\n",
            "Starting epoch 17\n",
            "0.3180425393358185 0.898361948595956\n",
            "Starting epoch 18\n",
            "0.31685393657705563 0.8963407801968591\n",
            "Starting epoch 19\n",
            "0.31665695957445666 0.9015369929840892\n",
            "Starting epoch 20\n",
            "0.3130277685980807 0.9005942838427194\n",
            "Starting epoch 21\n",
            "0.30852113920260216 0.897754305217976\n",
            "Starting epoch 22\n",
            "0.30576365131314387 0.9003410991018943\n",
            "Starting epoch 23\n",
            "0.30464922507136155 0.9013161296995399\n",
            "Starting epoch 24\n",
            "0.30199656496668015 0.9000825274516902\n",
            "Starting epoch 25\n",
            "0.30097071702957956 0.905599800037924\n",
            "Starting epoch 26\n",
            "0.2967745954380751 0.9005716588233266\n",
            "Starting epoch 27\n",
            "0.29350558376520974 0.9030916550309422\n",
            "Starting epoch 28\n",
            "0.2960170259208294 0.9053681629346159\n",
            "Starting epoch 29\n",
            "0.2921069335343425 0.9048391684335728\n",
            "Starting epoch 30\n",
            "0.2897232579197852 0.9060156694420025\n",
            "Starting epoch 31\n",
            "0.28745604882713505 0.9044804002689146\n",
            "Starting epoch 32\n",
            "0.2879185763302516 0.9034611970143596\n",
            "Starting epoch 33\n",
            "0.283216929884273 0.9013689214114564\n",
            "Starting epoch 34\n",
            "0.28113150447843116 0.9030776490665563\n",
            "Starting epoch 35\n",
            "0.2805488351317224 0.9036508162245092\n",
            "Starting epoch 36\n",
            "0.2791577363712632 0.9025852855492925\n",
            "Starting epoch 37\n",
            "0.2761015371755713 0.9044653169226526\n",
            "Starting epoch 38\n",
            "0.27354624332741845 0.8982369722983572\n",
            "Starting epoch 39\n",
            "0.2705919937202441 0.9011868438744376\n",
            "Starting epoch 40\n",
            "0.2699822162052927 0.9021381720708141\n",
            "Starting epoch 41\n",
            "0.2674337212559414 0.8990288479771078\n",
            "Starting epoch 42\n",
            "0.26423390121235174 0.9021952733102342\n",
            "Starting epoch 43\n",
            "0.2606310910712315 0.904078536829222\n",
            "Starting epoch 44\n",
            "0.26117634240878634 0.9007440399234629\n",
            "Starting epoch 45\n",
            "0.2588598777530527 0.8984524486735274\n",
            "Starting epoch 46\n",
            "0.25692498693247307 0.9024236782679147\n",
            "Starting epoch 47\n",
            "0.256027799649326 0.8992540207891607\n",
            "Starting epoch 48\n",
            "0.2532922346174661 0.8919698419265312\n",
            "Starting epoch 49\n",
            "0.25078653540261653 0.8958861250452501\n",
            "Starting epoch 50\n",
            "0.2526124062256087 0.89915813380221\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "model = MLPA(5, 128)   \n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val)).detach().numpy().ravel()\n",
        "\n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lNCW-mvBHAV7"
      },
      "source": [
        "Next, we use GPU for the deep learning and use early stop, otherwise, it's so slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6JQCDDGxB26-"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HcR1T2JpA2ud",
        "outputId": "bd9d0f79-ed79-4540-bcb6-26cbb3ffd765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5419350834320938 0.8677847735774249\n",
            "Starting epoch 2\n",
            "0.41497849388211205 0.8825783041147368\n",
            "Starting epoch 3\n",
            "0.38423603255034305 0.8934469324783231\n",
            "Starting epoch 4\n",
            "0.36513011513404386 0.8968379919325644\n",
            "Starting epoch 5\n",
            "0.3525338358253176 0.8997528485976797\n",
            "Starting epoch 6\n",
            "0.34415410942384883 0.9032823516229681\n",
            "Starting epoch 7\n",
            "0.34057416508251176 0.9060986278464429\n",
            "Starting epoch 8\n",
            "0.3344981500843262 0.9031821551085137\n",
            "Starting epoch 9\n",
            "0.3331608005568807 0.9019485528606644\n",
            "Starting epoch 10\n",
            "0.324282962247168 0.9027339642481599\n",
            "Starting epoch 11\n",
            "0.3215366905157406 0.9060953957008154\n",
            "Starting epoch 12\n",
            "0.31485509750490986 0.9028934167657857\n",
            "Starting epoch 13\n",
            "0.31253100049366717 0.9037434710658324\n",
            "Starting epoch 14\n",
            "0.3098059994666816 0.9032909706779749\n",
            "Starting epoch 15\n",
            "0.30339517120371623 0.9043327989519228\n",
            "Starting epoch 16\n",
            "0.30551535842660343 0.905431728465291\n",
            "Starting epoch 17\n",
            "0.3004792534390948 0.9040268224991812\n",
            "Starting epoch 18\n",
            "0.29602817308143076 0.9045030252883073\n",
            "Starting epoch 19\n",
            "0.2891935170041002 0.9051376032131837\n",
            "Starting epoch 20\n",
            "0.2874369870809759 0.9003163193187498\n",
            "Starting epoch 21\n",
            "0.28010846785275956 0.8872907724397098\n",
            "Starting epoch 22\n",
            "0.2849510981674981 0.9028524762545034\n",
            "Starting epoch 23\n",
            "0.27636084511928677 0.8967027805071452\n",
            "Starting epoch 24\n",
            "0.2661392722819046 0.8882561066004722\n",
            "Starting epoch 25\n",
            "0.27007833692653793 0.8992691041354226\n",
            "Starting epoch 26\n",
            "0.2619067376395084 0.8934824860802261\n",
            "Starting epoch 27\n",
            "0.2531833338913435 0.8904776680284772\n",
            "Starting epoch 28\n",
            "0.25334386723828756 0.8889790298391684\n",
            "Starting epoch 29\n",
            "0.24740604375689457 0.8903591560221337\n",
            "Starting epoch 30\n",
            "0.24135612754206742 0.8890038096223132\n",
            "Starting epoch 31\n",
            "0.23801399116704985 0.8927056937477374\n",
            "Starting epoch 32\n",
            "0.2421232185369257 0.8908482873937701\n",
            "Starting epoch 33\n",
            "0.23348115587591622 0.8920010860009309\n",
            "Starting epoch 34\n",
            "0.2245832523211172 0.8902234059057764\n",
            "Starting epoch 35\n",
            "0.2259448717324924 0.8865333729809864\n",
            "Starting epoch 36\n",
            "0.21578168349770366 0.8851252348692489\n",
            "Starting epoch 37\n",
            "0.21595936143680927 0.8916347761631416\n",
            "Starting epoch 38\n",
            "0.2031409046529374 0.8887344641533502\n",
            "Starting epoch 39\n",
            "0.20868565974153025 0.882049309613694\n",
            "Starting epoch 40\n",
            "0.2037148615625119 0.8897547447897811\n",
            "Starting epoch 41\n",
            "0.197686118910886 0.8822723276619951\n",
            "Starting epoch 42\n",
            "0.19805528816406281 0.8880083087690265\n",
            "Starting epoch 43\n",
            "0.18913429058192877 0.88277223285239\n",
            "Starting epoch 44\n",
            "0.1879947913546183 0.880941761045319\n",
            "Starting epoch 45\n",
            "0.1793165793324907 0.8820019048111564\n",
            "Starting epoch 46\n",
            "0.1728345488946587 0.8691422747409975\n",
            "Starting epoch 47\n",
            "0.17460617001790166 0.8850218062091673\n",
            "Starting epoch 48\n",
            "0.1717051361650457 0.8823660598851943\n",
            "Starting epoch 49\n",
            "0.16298355785490595 0.8726739325300374\n",
            "Starting epoch 50\n",
            "0.17413472385906584 0.8857307234834773\n",
            "Starting epoch 51\n",
            "0.15368472233468566 0.8858018306872835\n",
            "Starting epoch 52\n",
            "0.15413079477918876 0.8795562479529744\n",
            "Starting epoch 53\n",
            "0.15851284151914163 0.8788667235524297\n",
            "Starting epoch 54\n",
            "0.14730624409523263 0.8725597300511972\n",
            "Starting epoch 55\n",
            "0.15049423441158613 0.8850196514454155\n",
            "Starting epoch 56\n",
            "0.14123078460200375 0.8858460033441933\n",
            "Starting epoch 57\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 500\n",
        "model = MLPC(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val).to(device)).to(torch.device('cpu')).detach().numpy().ravel()\n",
        "    \n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    if auc > auc_max:\n",
        "        auc_max = auc\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 50:\n",
        "        print(epoch - 50)\n",
        "        break\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-kwmk88CzRj",
        "outputId": "dd60ab04-c3b9-49d9-cf3d-0e83609968f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5166030426966928 0.8706387581665546\n",
            "Starting epoch 2\n",
            "0.4020478748270643 0.8901878523038733\n",
            "Starting epoch 3\n",
            "0.3776672711555072 0.8955058092430745\n",
            "Starting epoch 4\n",
            "0.3568720729712784 0.8967157090896554\n",
            "Starting epoch 5\n",
            "0.3463206197706872 0.9002247418593026\n",
            "Starting epoch 6\n",
            "0.3375417239788635 0.8990083777214666\n",
            "Starting epoch 7\n",
            "0.3360020741343048 0.9048402458154488\n",
            "Starting epoch 8\n",
            "0.3216515325677559 0.9082038320318561\n",
            "Starting epoch 9\n",
            "0.31925732628327363 0.9085130406302253\n",
            "Starting epoch 10\n",
            "0.31379827061116544 0.9072234145248316\n",
            "Starting epoch 11\n",
            "0.3079761597813856 0.9081187188636637\n",
            "Starting epoch 12\n",
            "0.30209097041845573 0.9074701349744014\n",
            "Starting epoch 13\n",
            "0.301900502622021 0.9061654255227457\n",
            "Starting epoch 14\n",
            "0.29413863798227435 0.9109059057764908\n",
            "Starting epoch 15\n",
            "0.28215846392108346 0.9093383151471273\n",
            "Starting epoch 16\n",
            "0.2813346222706476 0.9063194911309923\n",
            "Starting epoch 17\n",
            "0.27254713251328017 0.9005070159107756\n",
            "Starting epoch 18\n",
            "0.27013869036249977 0.894765109203427\n",
            "Starting epoch 19\n",
            "0.26573429604882653 0.9011297426350175\n",
            "Starting epoch 20\n",
            "0.26065837859906865 0.8983457878678183\n",
            "Starting epoch 21\n",
            "0.25551811212216025 0.8988575442588475\n",
            "Starting epoch 22\n",
            "0.24964374183368093 0.8966381375945942\n",
            "Starting epoch 23\n",
            "0.23844899153052507 0.888974720311665\n",
            "Starting epoch 24\n",
            "0.23964900913863604 0.8999446225715811\n",
            "Starting epoch 25\n",
            "0.22628413297036704 0.8860302356449637\n",
            "Starting epoch 26\n",
            "0.21577073454590845 0.885196342073055\n",
            "Starting epoch 27\n",
            "0.21474074575995147 0.8910874661702092\n",
            "Starting epoch 28\n",
            "0.20514614463898734 0.8836740014824775\n",
            "Starting epoch 29\n",
            "0.1962704308992424 0.8826547982279223\n",
            "Starting epoch 30\n",
            "0.19475860774003736 0.8895037148127078\n",
            "Starting epoch 31\n",
            "0.1936421151715727 0.8947548740756064\n",
            "Starting epoch 32\n",
            "0.17212015417867785 0.8894724707383083\n",
            "Starting epoch 33\n",
            "0.17887836781872152 0.877663287997104\n",
            "Starting epoch 34\n",
            "0.16633709929753066 0.890342995293996\n",
            "Starting epoch 35\n",
            "0.15751270051684707 0.8835996621330438\n",
            "Starting epoch 36\n",
            "0.1427102907293471 0.885949432004275\n",
            "Starting epoch 37\n",
            "0.14266607386620242 0.8800055161952043\n",
            "Starting epoch 38\n",
            "0.1324763355480952 0.8800841650721416\n",
            "Starting epoch 39\n",
            "0.14307362905064427 0.891145644791505\n",
            "Starting epoch 40\n",
            "0.13231392142451523 0.8803901415248834\n",
            "Starting epoch 41\n",
            "0.11677597495916078 0.879306295357777\n",
            "Starting epoch 42\n",
            "0.12399848315074855 0.8863523728258434\n",
            "Starting epoch 43\n",
            "0.1010123645383279 0.8820374584130596\n",
            "Starting epoch 44\n",
            "0.12549475651564615 0.8769813052696901\n",
            "Starting epoch 45\n",
            "0.09802209355996105 0.8734453379531466\n",
            "Starting epoch 46\n",
            "0.0985848776276782 0.8710632466256399\n",
            "Starting epoch 47\n",
            "0.10211990251134828 0.8860043784799435\n",
            "Starting epoch 48\n",
            "0.08701259169860356 0.8717301460067917\n",
            "Starting epoch 49\n",
            "0.07932058362788875 0.8825028873834273\n",
            "Starting epoch 50\n",
            "0.07571184245235021 0.8822378514419679\n",
            "Starting epoch 51\n",
            "0.059288521123856276 0.8787406698729553\n",
            "Starting epoch 52\n",
            "0.0711571647000793 0.8708413059592146\n",
            "Starting epoch 53\n",
            "0.07281570171311197 0.8802091413697404\n",
            "Starting epoch 54\n",
            "0.06276476559606287 0.8840392339383909\n",
            "Starting epoch 55\n",
            "0.07725704775031574 0.8733957783868578\n",
            "Starting epoch 56\n",
            "0.07177890466847808 0.8808130139111547\n",
            "Starting epoch 57\n",
            "0.0682603103411115 0.8787110418713692\n",
            "Starting epoch 58\n",
            "0.06159745741685808 0.8819081725879575\n",
            "Starting epoch 59\n",
            "0.057780986190520904 0.8867057540811224\n",
            "Starting epoch 60\n",
            "0.054216870737208135 0.8783991398183102\n",
            "Starting epoch 61\n",
            "0.05416991042671504 0.8801304924928032\n",
            "Starting epoch 62\n",
            "0.04556250486759556 0.8856790091534366\n",
            "Starting epoch 63\n",
            "0.04566245695548231 0.8730494001137714\n",
            "Starting epoch 64\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 500\n",
        "model = MLPA(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val).to(device)).to(torch.device('cpu')).detach().numpy().ravel()\n",
        "    \n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    if auc > auc_max:\n",
        "        auc_max = auc\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 50:\n",
        "        print(epoch - 50)\n",
        "        break\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7etXOIWDxF5",
        "outputId": "968a6176-0bd5-46e1-8473-bd0eb0513962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.51760094067879 0.8709447346192964\n",
            "Starting epoch 2\n",
            "0.4116401991372807 0.8442191998069332\n",
            "Starting epoch 3\n",
            "0.3863207785059928 0.8842525555498095\n",
            "Starting epoch 4\n",
            "0.3646212212307906 0.8995330626950061\n",
            "Starting epoch 5\n",
            "0.34942906661802237 0.9044071383013566\n",
            "Starting epoch 6\n",
            "0.3389652813954341 0.902761976176932\n",
            "Starting epoch 7\n",
            "0.3289522474725452 0.8992023064591198\n",
            "Starting epoch 8\n",
            "0.32460355557834136 0.905620270293565\n",
            "Starting epoch 9\n",
            "0.3222736582058347 0.9067202771888091\n",
            "Starting epoch 10\n",
            "0.31701285878589425 0.9095042319560083\n",
            "Starting epoch 11\n",
            "0.3062646071229106 0.9070434917515644\n",
            "Starting epoch 12\n",
            "0.30268339731397104 0.905714002516764\n",
            "Starting epoch 13\n",
            "0.3035432392966547 0.9050147816793367\n",
            "Starting epoch 14\n",
            "0.2916056577552947 0.8970798641636932\n",
            "Starting epoch 15\n",
            "0.28577531317708793 0.9039901915154023\n",
            "Starting epoch 16\n",
            "0.27865626991645787 0.9036863698264123\n",
            "Starting epoch 17\n",
            "0.2781891570168067 0.8958872024271259\n",
            "Starting epoch 18\n",
            "0.26367881266709825 0.8995233662581235\n",
            "Starting epoch 19\n",
            "0.26176736848019366 0.8969161021185637\n",
            "Starting epoch 20\n",
            "0.2510086913779783 0.8999887952284912\n",
            "Starting epoch 21\n",
            "0.24517168785762722 0.8993423661029805\n",
            "Starting epoch 22\n",
            "0.23688302845505305 0.8916466273637759\n",
            "Starting epoch 23\n",
            "0.22749578193463257 0.8904388822809468\n",
            "Starting epoch 24\n",
            "0.22196173651438006 0.8970389236524108\n",
            "Starting epoch 25\n",
            "0.2176702870823346 0.8805571357156401\n",
            "Starting epoch 26\n",
            "0.2106249292949296 0.8886482736032821\n",
            "Starting epoch 27\n",
            "0.2039807723945606 0.8928953129578872\n",
            "Starting epoch 28\n",
            "0.1897942962858207 0.889496173139577\n",
            "Starting epoch 29\n",
            "0.18206905796036876 0.8960692799641445\n",
            "Starting epoch 30\n",
            "0.17563412539062012 0.8908191980831223\n",
            "Starting epoch 31\n",
            "0.162715746147596 0.8865301408353589\n",
            "Starting epoch 32\n",
            "0.16446982927685241 0.8854786161245282\n",
            "Starting epoch 33\n",
            "0.16461890030109733 0.877482287841961\n",
            "Starting epoch 34\n",
            "0.14595842786844557 0.8834843822723277\n",
            "Starting epoch 35\n",
            "0.13723987147662553 0.8839153350226681\n",
            "Starting epoch 36\n",
            "0.128726974626425 0.8762012807915741\n",
            "Starting epoch 37\n",
            "0.12557292716819557 0.8690657806278119\n",
            "Starting epoch 38\n",
            "0.11309781676293729 0.8899815336746479\n",
            "Starting epoch 39\n",
            "0.1126066641805673 0.8806390167382048\n",
            "Starting epoch 40\n",
            "0.11629935579899026 0.8791199082932546\n",
            "Starting epoch 41\n",
            "0.11545011431844607 0.8852599076037303\n",
            "Starting epoch 42\n",
            "0.0926120507436452 0.876960835014049\n",
            "Starting epoch 43\n",
            "0.09024393282265133 0.884900062057196\n",
            "Starting epoch 44\n",
            "0.08972697377699472 0.8840861000499904\n",
            "Starting epoch 45\n",
            "0.080556762840813 0.8835145489648515\n",
            "Starting epoch 46\n",
            "0.07807391357371776 0.8723916584785644\n",
            "Starting epoch 47\n",
            "0.08298521735000579 0.8772054006998673\n",
            "Starting epoch 48\n",
            "0.06836337279943497 0.8786189257209839\n",
            "Starting epoch 49\n",
            "0.0584189540406746 0.8801682008584578\n",
            "Starting epoch 50\n",
            "0.06447587598522679 0.8759103876850942\n",
            "Starting epoch 51\n",
            "0.07158338734972283 0.8781093240937063\n",
            "Starting epoch 52\n",
            "0.08743644628723116 0.8747500474048024\n",
            "Starting epoch 53\n",
            "0.06002209524201804 0.875665283308338\n",
            "Starting epoch 54\n",
            "0.055654511334023986 0.8795508610435951\n",
            "Starting epoch 55\n",
            "0.05816029327635446 0.8789453724293669\n",
            "Starting epoch 56\n",
            "0.05621838252508707 0.8800270638327213\n",
            "Starting epoch 57\n",
            "0.0644078358593028 0.8839999094999225\n",
            "Starting epoch 58\n",
            "0.07287205124593796 0.879827748185689\n",
            "Starting epoch 59\n",
            "0.054369926556263354 0.8805614452431436\n",
            "Starting epoch 60\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 500\n",
        "model = MLPA(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val).to(device)).to(torch.device('cpu')).detach().numpy().ravel()\n",
        "    \n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    if auc > auc_max:\n",
        "        auc_max = auc\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 50:\n",
        "        print(epoch - 50)\n",
        "        break\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xc383rb2HM0e"
      },
      "source": [
        "From the results, auc first goes up and then goes down, so it's overfitting.\n",
        "We try to add the batchnorm and dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "DkO0SDPGEbiG"
      },
      "outputs": [],
      "source": [
        "class MLPD(nn.Module):\n",
        "    def __init__(self,n_layer,n_hidden_nodes):\n",
        "        super(MLPD, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in,n_hidden_nodes))\n",
        "        self.layers.append(nn.BatchNorm1d(n_hidden_nodes))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Dropout(0.2))\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Linear(n_hidden_nodes,n_hidden_nodes))\n",
        "            self.layers.append(nn.BatchNorm1d(n_hidden_nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(0.2))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes,d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqPTm7ViE3xw",
        "outputId": "beab3ac1-87ac-44ee-eb2c-c2b05d0e24cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.6048369246124785 0.6780395097481512\n",
            "Starting epoch 2\n",
            "0.537072368294248 0.757888590094982\n",
            "Starting epoch 3\n",
            "0.49472445539550974 0.8100144800124113\n",
            "Starting epoch 4\n",
            "0.4772448772370117 0.82612026167451\n",
            "Starting epoch 5\n",
            "0.46506023217056647 0.8334464584302977\n",
            "Starting epoch 6\n",
            "0.4597622203435312 0.8459387012807916\n",
            "Starting epoch 7\n",
            "0.44120245334297215 0.8480083518643016\n",
            "Starting epoch 8\n",
            "0.4349439652254841 0.8605232197341883\n",
            "Starting epoch 9\n",
            "0.42888385949454094 0.875948096050749\n",
            "Starting epoch 10\n",
            "0.4169088445046873 0.8798299029494406\n",
            "Starting epoch 11\n",
            "0.42330774276305966 0.8811777076761304\n",
            "Starting epoch 12\n",
            "0.4097566038505829 0.872178336867146\n",
            "Starting epoch 13\n",
            "0.40933619906390234 0.880587302408164\n",
            "Starting epoch 14\n",
            "0.4039662671905389 0.880682112013239\n",
            "Starting epoch 15\n",
            "0.4030266058590714 0.8807273620520246\n",
            "Starting epoch 16\n",
            "0.40551377979225073 0.8859688248780404\n",
            "Starting epoch 17\n",
            "0.3965710679598851 0.8849722466428781\n",
            "Starting epoch 18\n",
            "0.39638535663219227 0.8922079433210944\n",
            "Starting epoch 19\n",
            "0.39218870310406784 0.8853698005550671\n",
            "Starting epoch 20\n",
            "0.3910918084569603 0.8898473996311045\n",
            "Starting epoch 21\n",
            "0.3928425417223407 0.8885523866163314\n",
            "Starting epoch 22\n",
            "0.3916496431254709 0.8891858871593318\n",
            "Starting epoch 23\n",
            "0.39053218494324393 0.8889714881660375\n",
            "Starting epoch 24\n",
            "0.38375993196635316 0.8925473186119872\n",
            "Starting epoch 25\n",
            "0.3887637743617088 0.8933564324007517\n",
            "Starting epoch 26\n",
            "0.38580411226447175 0.8948259812794125\n",
            "Starting epoch 27\n",
            "0.3818621525342423 0.8924762114081811\n",
            "Starting epoch 28\n",
            "0.37860996885478015 0.8863706883177329\n",
            "Starting epoch 29\n",
            "0.3811514851016218 0.8892451431625037\n",
            "Starting epoch 30\n",
            "0.37457788096080186 0.8894530778645431\n",
            "Starting epoch 31\n",
            "0.37755961604818006 0.8981378531657789\n",
            "Starting epoch 32\n",
            "0.3785448057650374 0.8890856906448777\n",
            "Starting epoch 33\n",
            "0.3724937928228828 0.9043802037544604\n",
            "Starting epoch 34\n",
            "0.38116900987602276 0.8884866663219045\n",
            "Starting epoch 35\n",
            "0.37156891391999486 0.8987487286893865\n",
            "Starting epoch 36\n",
            "0.3714411065336644 0.8941429211701231\n",
            "Starting epoch 37\n",
            "0.37295941463857163 0.9024667735429487\n",
            "Starting epoch 38\n",
            "0.36441903316030677 0.8908956921963075\n",
            "Starting epoch 39\n",
            "0.373959084795144 0.8943777904190584\n",
            "Starting epoch 40\n",
            "0.366296266059715 0.8976007783006671\n",
            "Starting epoch 41\n",
            "0.3709029477328404 0.8961565478960887\n",
            "Starting epoch 42\n",
            "0.3662347911682541 0.892536544793229\n",
            "Starting epoch 43\n",
            "0.3623733438979307 0.8959615417765596\n",
            "Starting epoch 44\n",
            "0.36354080938314653 0.8958839702814982\n",
            "Starting epoch 45\n",
            "0.3591446162400698 0.8946859216355518\n",
            "Starting epoch 46\n",
            "0.3625532096745337 0.9021004637051594\n",
            "Starting epoch 47\n",
            "0.3670563441487866 0.8983781093240937\n",
            "Starting epoch 48\n",
            "0.36136992565640047 0.8946654513799107\n",
            "Starting epoch 49\n",
            "0.35487411224819787 0.9004887004188861\n",
            "Starting epoch 50\n",
            "0.3646506387902864 0.8953786781817242\n",
            "Starting epoch 51\n",
            "0.35492820548865345 0.8999790987916084\n",
            "Starting epoch 52\n",
            "0.3547931528494196 0.8997560807433074\n",
            "Starting epoch 53\n",
            "0.35726338513119027 0.8970400010342867\n",
            "Starting epoch 54\n",
            "0.35381059109618307 0.898093680508869\n",
            "Starting epoch 55\n",
            "0.3548604204948915 0.9039417093309889\n",
            "Starting epoch 56\n",
            "0.35394225388693396 0.8988489252038407\n",
            "Starting epoch 57\n",
            "0.359281910153848 0.8983080795021633\n",
            "Starting epoch 58\n",
            "0.34921890823403556 0.8984093533984934\n",
            "Starting epoch 59\n",
            "0.3493799509176579 0.8991268897278103\n",
            "Starting epoch 60\n",
            "0.35607378916563176 0.8940330282187862\n",
            "Starting epoch 61\n",
            "0.35693487236490185 0.8965971970833118\n",
            "Starting epoch 62\n",
            "0.3497967523157886 0.9026251486786989\n",
            "Starting epoch 63\n",
            "0.3485494643443817 0.8962416610642809\n",
            "Starting epoch 64\n",
            "0.34694701192769617 0.8941353794969918\n",
            "Starting epoch 65\n",
            "0.3486242388363539 0.9013840047577184\n",
            "Starting epoch 66\n",
            "0.34192861023452564 0.9029634465877161\n",
            "Starting epoch 67\n",
            "0.34684553993441986 0.9007052541759322\n",
            "Starting epoch 68\n",
            "0.34220089567356127 0.9023536484459845\n",
            "Starting epoch 69\n",
            "0.344368005019636 0.8977974004930098\n",
            "Starting epoch 70\n",
            "0.3388976092873692 0.8954702556411713\n",
            "Starting epoch 71\n",
            "0.3445433099922569 0.903579709020703\n",
            "Starting epoch 72\n",
            "0.3475182922171459 0.8925009911913259\n",
            "Starting epoch 73\n",
            "0.3349472517325621 0.899498586474979\n",
            "Starting epoch 74\n",
            "0.3431678823017001 0.9008744031304406\n",
            "Starting epoch 75\n",
            "0.341110208846879 0.898556954715485\n",
            "Starting epoch 76\n",
            "0.34427892982927644 0.9008334626191584\n",
            "Starting epoch 77\n",
            "0.3353755759618855 0.8905897157435658\n",
            "Starting epoch 78\n",
            "0.3373625814582005 0.8973028822119944\n",
            "Starting epoch 79\n",
            "0.3368405807547424 0.8978738946061955\n",
            "Starting epoch 80\n",
            "0.3334802316889999 0.8928209736084536\n",
            "Starting epoch 81\n",
            "0.33836295104407454 0.8963623278343763\n",
            "Starting epoch 82\n",
            "0.33677607029130285 0.8940319508369101\n",
            "Starting epoch 83\n",
            "32\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 500\n",
        "model = MLPD(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val).to(device)).to(torch.device('cpu')).detach().numpy().ravel()\n",
        "    \n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    if auc > auc_max:\n",
        "        auc_max = auc\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 50:\n",
        "        print(epoch - 50)\n",
        "        break\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_AWyTTQHIEDS"
      },
      "source": [
        "The overfitting is better, try several more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzGYsZMAE6Rq",
        "outputId": "98580db3-9bfb-47e3-d72f-1cbba12d5a0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5286637745684493 0.8628589836410336\n",
            "Starting epoch 2\n",
            "0.40995539821115096 0.8783409611970144\n",
            "Starting epoch 3\n",
            "0.3852384265125964 0.887738963300064\n",
            "Starting epoch 4\n",
            "0.3643564077705871 0.8914483890986193\n",
            "Starting epoch 5\n",
            "0.349073428848497 0.8956200117219147\n",
            "Starting epoch 6\n",
            "0.340473740694489 0.8983382461946872\n",
            "Starting epoch 7\n",
            "0.33331514763071096 0.898759502508145\n",
            "Starting epoch 8\n",
            "0.32808206472310825 0.9010855699781076\n",
            "Starting epoch 9\n",
            "0.323097426885305 0.9008022185447588\n",
            "Starting epoch 10\n",
            "0.3179785347369096 0.9028190774163521\n",
            "Starting epoch 11\n",
            "0.31162201698828895 0.9044308407026254\n",
            "Starting epoch 12\n",
            "0.3088135482495174 0.9026316129699539\n",
            "Starting epoch 13\n",
            "0.30472876120387243 0.9066276223474858\n",
            "Starting epoch 14\n",
            "0.3021124049165192 0.9054769785040768\n",
            "Starting epoch 15\n",
            "0.29894460924212174 0.9026908689731259\n",
            "Starting epoch 16\n",
            "0.29164100479292776 0.9030464049921567\n",
            "Starting epoch 17\n",
            "0.28892645988933136 0.9029828394614815\n",
            "Starting epoch 18\n",
            "0.2871040163521984 0.9036680543345228\n",
            "Starting epoch 19\n",
            "0.28096247815194714 0.9031929289272724\n",
            "Starting epoch 20\n",
            "0.27615321553606076 0.9026294582062022\n",
            "Starting epoch 21\n",
            "0.27339073907585826 0.9036335781144956\n",
            "Starting epoch 22\n",
            "0.2679769663378151 0.9023461067728533\n",
            "Starting epoch 23\n",
            "0.26660678750105965 0.9026262260605746\n",
            "Starting epoch 24\n",
            "0.262920570068672 0.9017513919773835\n",
            "Starting epoch 25\n",
            "0.2561944220001064 0.8955844581200118\n",
            "Starting epoch 26\n",
            "0.25997234856458007 0.9050255554980952\n",
            "Starting epoch 27\n",
            "0.24896087518227336 0.8951535053696712\n",
            "Starting epoch 28\n",
            "0.24536719880727795 0.8962966075399492\n",
            "Starting epoch 29\n",
            "0.24517778940678286 0.8993768423230075\n",
            "Starting epoch 30\n",
            "0.2380923551837579 0.9002096585130406\n",
            "Starting epoch 31\n",
            "0.23395744946387834 0.8974149299270828\n",
            "Starting epoch 32\n",
            "0.23083299392485496 0.8933865990932754\n",
            "Starting epoch 33\n",
            "0.2209154072580927 0.8923954077674924\n",
            "Starting epoch 34\n",
            "0.2202136507762243 0.8901242867731982\n",
            "Starting epoch 35\n",
            "0.20982533140362244 0.8925386995569806\n",
            "Starting epoch 36\n",
            "0.20742963356816754 0.8967135543259037\n",
            "Starting epoch 37\n",
            "0.1953095246785473 0.8903860905690301\n",
            "Starting epoch 38\n",
            "0.1978578535620072 0.886934159038803\n",
            "Starting epoch 39\n",
            "0.18405902246755837 0.8940750461119442\n",
            "Starting epoch 40\n",
            "0.18314524602574958 0.8920043181465583\n",
            "Starting epoch 41\n",
            "0.17827825406857353 0.8803513557773526\n",
            "Starting epoch 42\n",
            "0.1776791279477921 0.8740982313699126\n",
            "Starting epoch 43\n",
            "0.17647863691347437 0.8831988760752272\n",
            "Starting epoch 44\n",
            "0.16470122080004415 0.881839220147903\n",
            "Starting epoch 45\n",
            "0.15852491273869457 0.8843215079898639\n",
            "Starting epoch 46\n",
            "0.15508176047302638 0.8874146713554325\n",
            "Starting epoch 47\n",
            "0.14919152685196008 0.8818306010928961\n",
            "Starting epoch 48\n",
            "0.15046970274654722 0.8867316112461431\n",
            "Starting epoch 49\n",
            "0.1412303142758012 0.8797329385806141\n",
            "Starting epoch 50\n",
            "0.1260489534962571 0.8801197186740446\n",
            "Starting epoch 51\n",
            "0.1330830792580446 0.8742307493406425\n",
            "Starting epoch 52\n",
            "0.1323377017164136 0.878915205736843\n",
            "Starting epoch 53\n",
            "0.12281566361615885 0.8808415645308648\n",
            "Starting epoch 54\n",
            "0.12011051074075908 0.8780263656892658\n",
            "Starting epoch 55\n",
            "0.1194206449836237 0.8713369016221062\n",
            "Starting epoch 56\n",
            "0.10863505012395833 0.8725726586337074\n",
            "Starting epoch 57\n",
            "0.10914535908109595 0.868906328110186\n",
            "Starting epoch 58\n",
            "0.1074537155750276 0.8775415438451328\n",
            "Starting epoch 59\n",
            "0.10150985316872717 0.8789755391218907\n",
            "Starting epoch 60\n",
            "0.09257677134481092 0.8755871731223388\n",
            "Starting epoch 61\n",
            "0.09040177308197113 0.8772312578648876\n",
            "Starting epoch 62\n",
            "0.0925937259763239 0.8791015928013651\n",
            "Starting epoch 63\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 500\n",
        "model = MLPA(10, 256)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val).to(device)).to(torch.device('cpu')).detach().numpy().ravel()\n",
        "    \n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    if auc > auc_max:\n",
        "        auc_max = auc\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 50:\n",
        "        print(epoch - 50)\n",
        "        break\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6LHxyyJeIelV"
      },
      "source": [
        "We choose the coefficients from the highest auc score to run and save a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "9AnE1-EyKYy7"
      },
      "outputs": [],
      "source": [
        "Allloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d90ULO2YKN0l",
        "outputId": "56cb9b0f-dcff-4cf7-c6eb-57e536e7f6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Starting epoch 2\n",
            "Starting epoch 3\n",
            "Starting epoch 4\n",
            "Starting epoch 5\n",
            "Starting epoch 6\n",
            "Starting epoch 7\n",
            "Starting epoch 8\n",
            "Starting epoch 9\n",
            "Starting epoch 10\n",
            "Starting epoch 11\n",
            "Starting epoch 12\n",
            "Starting epoch 13\n",
            "Starting epoch 14\n",
            "Starting epoch 15\n",
            "Starting epoch 16\n",
            "Starting epoch 17\n",
            "Starting epoch 18\n",
            "Starting epoch 19\n",
            "Starting epoch 20\n",
            "Starting epoch 21\n",
            "Starting epoch 22\n",
            "Starting epoch 23\n",
            "Starting epoch 24\n",
            "Starting epoch 25\n",
            "Starting epoch 26\n",
            "Starting epoch 27\n",
            "Starting epoch 28\n",
            "Starting epoch 29\n",
            "Starting epoch 30\n",
            "Starting epoch 31\n",
            "Starting epoch 32\n",
            "Starting epoch 33\n",
            "Starting epoch 34\n",
            "Starting epoch 35\n",
            "Starting epoch 36\n",
            "Starting epoch 37\n",
            "Starting epoch 38\n",
            "Starting epoch 39\n",
            "Starting epoch 40\n",
            "Starting epoch 41\n",
            "Starting epoch 42\n",
            "Starting epoch 43\n",
            "Starting epoch 44\n",
            "Starting epoch 45\n",
            "Starting epoch 46\n",
            "Starting epoch 47\n",
            "Starting epoch 48\n",
            "Starting epoch 49\n",
            "Starting epoch 50\n",
            "Starting epoch 51\n",
            "Starting epoch 52\n",
            "Starting epoch 53\n",
            "Starting epoch 54\n",
            "Starting epoch 55\n",
            "Starting epoch 56\n",
            "Starting epoch 57\n",
            "Starting epoch 58\n",
            "Starting epoch 59\n",
            "Starting epoch 60\n",
            "Starting epoch 61\n",
            "Starting epoch 62\n",
            "Starting epoch 63\n",
            "Starting epoch 64\n",
            "Starting epoch 65\n",
            "Starting epoch 66\n",
            "Starting epoch 67\n",
            "Starting epoch 68\n",
            "Starting epoch 69\n",
            "Starting epoch 70\n",
            "Starting epoch 71\n",
            "Starting epoch 72\n",
            "Starting epoch 73\n",
            "Starting epoch 74\n",
            "Starting epoch 75\n",
            "Starting epoch 76\n",
            "Starting epoch 77\n",
            "Starting epoch 78\n",
            "Starting epoch 79\n",
            "Starting epoch 80\n",
            "Starting epoch 81\n",
            "Starting epoch 82\n",
            "Starting epoch 83\n",
            "Starting epoch 84\n",
            "Starting epoch 85\n",
            "Starting epoch 86\n",
            "Starting epoch 87\n",
            "Starting epoch 88\n",
            "Starting epoch 89\n",
            "Starting epoch 90\n",
            "Starting epoch 91\n",
            "Starting epoch 92\n",
            "Starting epoch 93\n",
            "Starting epoch 94\n",
            "Starting epoch 95\n",
            "Starting epoch 96\n",
            "Starting epoch 97\n",
            "Starting epoch 98\n",
            "Starting epoch 99\n",
            "Starting epoch 100\n",
            "Starting epoch 101\n",
            "Starting epoch 102\n",
            "Starting epoch 103\n",
            "Starting epoch 104\n",
            "Starting epoch 105\n",
            "Starting epoch 106\n",
            "Starting epoch 107\n",
            "Starting epoch 108\n",
            "Starting epoch 109\n",
            "Starting epoch 110\n",
            "Starting epoch 111\n",
            "Starting epoch 112\n",
            "Starting epoch 113\n",
            "Starting epoch 114\n",
            "Starting epoch 115\n",
            "Starting epoch 116\n",
            "Starting epoch 117\n",
            "Starting epoch 118\n",
            "Starting epoch 119\n",
            "Starting epoch 120\n",
            "Starting epoch 121\n",
            "Starting epoch 122\n",
            "Starting epoch 123\n",
            "Starting epoch 124\n",
            "Starting epoch 125\n",
            "Starting epoch 126\n",
            "Starting epoch 127\n",
            "Starting epoch 128\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 128\n",
        "model = MLPD(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    for i, data in enumerate(Allloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfazYYhhNcXQ",
        "outputId": "5d62f0c8-a636-4d2c-a542-f176b81bcfc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLPD(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=512, bias=True)\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.2, inplace=False)\n",
              "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): Dropout(p=0.2, inplace=False)\n",
              "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU()\n",
              "    (15): Dropout(p=0.2, inplace=False)\n",
              "    (16): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): ReLU()\n",
              "    (19): Dropout(p=0.2, inplace=False)\n",
              "    (20): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (21): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU()\n",
              "    (23): Dropout(p=0.2, inplace=False)\n",
              "    (24): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (25): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU()\n",
              "    (27): Dropout(p=0.2, inplace=False)\n",
              "    (28): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (29): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (30): ReLU()\n",
              "    (31): Dropout(p=0.2, inplace=False)\n",
              "    (32): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (33): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (34): ReLU()\n",
              "    (35): Dropout(p=0.2, inplace=False)\n",
              "    (36): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (37): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (38): ReLU()\n",
              "    (39): Dropout(p=0.2, inplace=False)\n",
              "    (40): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (41): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU()\n",
              "    (43): Dropout(p=0.2, inplace=False)\n",
              "    (44): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (45): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (46): ReLU()\n",
              "    (47): Dropout(p=0.2, inplace=False)\n",
              "    (48): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (49): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (50): ReLU()\n",
              "    (51): Dropout(p=0.2, inplace=False)\n",
              "    (52): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (53): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (54): ReLU()\n",
              "    (55): Dropout(p=0.2, inplace=False)\n",
              "    (56): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (57): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (58): ReLU()\n",
              "    (59): Dropout(p=0.2, inplace=False)\n",
              "    (60): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (61): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (62): ReLU()\n",
              "    (63): Dropout(p=0.2, inplace=False)\n",
              "    (64): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dIZ2E11KKN4D"
      },
      "outputs": [],
      "source": [
        "model.to(torch.device('cpu'))\n",
        "# save the model: you must use the .pth format for pytorch models!\n",
        "model_savepath = 'MLP1.pth'\n",
        "\n",
        "# To save a PyTorch model, we first pass an input through the model, \n",
        "# and then save the \"trace\". \n",
        "# For this purpose, we can use any input. \n",
        "# We will create a random input with the proper dimension.\n",
        "x = torch.randn(d_in) # random input\n",
        "x = x[None,:] # add singleton batch index\n",
        "with torch.no_grad():\n",
        "    traced_cell = torch.jit.trace(model, (x))\n",
        "\n",
        "# Now we save the trace\n",
        "torch.jit.save(traced_cell, model_savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "6lv2Z8PFNgqU"
      },
      "outputs": [],
      "source": [
        "yts_hat_savepath = 'MLP1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShO-k5wYKN4D",
        "outputId": "36478d6c-41c1-40f6-f462-87411dce6aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training auc =  0.9401277162947026\n",
            "test label confidences saved in MLP1.csv\n"
          ]
        }
      ],
      "source": [
        "# generate kaggle submission file using the validation script\n",
        "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdeBAaPOObf2",
        "outputId": "02b04583-31c7-4f13-e340-72648c73abf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Starting epoch 2\n",
            "Starting epoch 3\n",
            "Starting epoch 4\n",
            "Starting epoch 5\n",
            "Starting epoch 6\n",
            "Starting epoch 7\n",
            "Starting epoch 8\n",
            "Starting epoch 9\n",
            "Starting epoch 10\n",
            "Starting epoch 11\n",
            "Starting epoch 12\n",
            "Starting epoch 13\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 13\n",
        "model = MLPA(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    for i, data in enumerate(Allloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj6lvqcHObf3",
        "outputId": "2a03ad4f-8cd4-44f1-e639-fdb0ea7e6431"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLPA(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=8, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (11): ReLU()\n",
              "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (13): ReLU()\n",
              "    (14): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (15): ReLU()\n",
              "    (16): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (17): ReLU()\n",
              "    (18): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (19): ReLU()\n",
              "    (20): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (21): ReLU()\n",
              "    (22): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (23): ReLU()\n",
              "    (24): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (25): ReLU()\n",
              "    (26): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (27): ReLU()\n",
              "    (28): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (29): ReLU()\n",
              "    (30): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (31): ReLU()\n",
              "    (32): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "aDBqWFNVObf3"
      },
      "outputs": [],
      "source": [
        "model.to(torch.device('cpu'))\n",
        "# save the model: you must use the .pth format for pytorch models!\n",
        "model_savepath = 'MLP2.pth'\n",
        "\n",
        "# To save a PyTorch model, we first pass an input through the model, \n",
        "# and then save the \"trace\". \n",
        "# For this purpose, we can use any input. \n",
        "# We will create a random input with the proper dimension.\n",
        "x = torch.randn(d_in) # random input\n",
        "x = x[None,:] # add singleton batch index\n",
        "with torch.no_grad():\n",
        "    traced_cell = torch.jit.trace(model, (x))\n",
        "\n",
        "# Now we save the trace\n",
        "torch.jit.save(traced_cell, model_savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5cM4hwMYObf4"
      },
      "outputs": [],
      "source": [
        "yts_hat_savepath = 'MLP2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1Qdh9bAObf4",
        "outputId": "4628ebd6-1be5-490b-8c6c-96a772051139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training auc =  0.9335692906049922\n",
            "test label confidences saved in MLP2.csv\n"
          ]
        }
      ],
      "source": [
        "# generate kaggle submission file using the validation script\n",
        "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oBZmyYt2ImS6"
      },
      "source": [
        "The result is worse than expect, the score is low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DBhLCTEAg6B"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "lBk74bNYiDPu"
      },
      "outputs": [],
      "source": [
        "# create a model\n",
        "d_in = Xtr.shape[1]\n",
        "d_out = 1\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_layer, n_hidden_nodes, n_channels, kernel_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in, n_hidden_nodes))\n",
        "        self.layers.append(nn.Conv1d(1, n_channels, kernel_size))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Conv1d(n_channels, n_channels, kernel_size))\n",
        "            self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Flatten())\n",
        "        self.layers.append(nn.Linear(n_channels * (n_hidden_nodes - (kernel_size - 1) * (n_layer + 1)), n_hidden_nodes))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes, d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out\n",
        "\n",
        "# Usually, we would train the model at this point. \n",
        "# But this is only a demo, so we'll use the randomly initialized weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XLAcgUU_jtgV"
      },
      "outputs": [],
      "source": [
        "model = CNN(5, 128, 64, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bl44PKRk1bN",
        "outputId": "8dfdff45-e907-4157-db79-ca2431e4c2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5368286321326912 0.8468447794383824\n",
            "Starting epoch 2\n",
            "0.42595150380100005 0.8716590388029857\n",
            "Starting epoch 3\n",
            "0.40394386922120745 0.8781200979124648\n",
            "Starting epoch 4\n",
            "0.3960970289185548 0.8787557532192172\n",
            "Starting epoch 5\n",
            "0.38404036774585393 0.8800949388909001\n",
            "Starting epoch 6\n",
            "0.37915191499410444 0.8865710813466412\n",
            "Starting epoch 7\n",
            "0.3766768257320468 0.8869276947475478\n",
            "Starting epoch 8\n",
            "0.36972685559748036 0.8898958818155178\n",
            "Starting epoch 9\n",
            "0.3649458913305592 0.8914677819723845\n",
            "Starting epoch 10\n",
            "0.3618112587723362 0.8937701470410785\n",
            "Starting epoch 11\n",
            "0.3591139923625429 0.8914710141180122\n",
            "Starting epoch 12\n",
            "0.3552314503991561 0.8919881574184207\n",
            "Starting epoch 13\n",
            "0.35211421315605795 0.8906866801123925\n",
            "Starting epoch 14\n",
            "0.34932493515977714 0.8920398717484614\n",
            "Starting epoch 15\n",
            "0.34498181184530974 0.8929211701229077\n",
            "Starting epoch 16\n",
            "0.3446531001006319 0.8932659323231802\n",
            "Starting epoch 17\n",
            "0.3406558523206521 0.8923652410749687\n",
            "Starting epoch 18\n",
            "0.3376515789573928 0.8956404819775559\n",
            "Starting epoch 19\n",
            "0.33604851570550415 0.8945684870110839\n",
            "Starting epoch 20\n",
            "0.3324463706966598 0.895270939994139\n",
            "Starting epoch 21\n",
            "0.3296381269577975 0.8946966954543105\n",
            "Starting epoch 22\n",
            "0.32791514355119733 0.898122769819517\n",
            "Starting epoch 23\n",
            "0.3263112943259757 0.8950457671820862\n",
            "Starting epoch 24\n",
            "0.32134618993819813 0.8929308665597906\n",
            "Starting epoch 25\n",
            "0.32131313788972105 0.8953571305442072\n",
            "Starting epoch 26\n",
            "0.3154289094459186 0.893374747892641\n",
            "Starting epoch 27\n",
            "0.31350673434877113 0.8907330075330541\n",
            "Starting epoch 28\n",
            "0.3122101209850256 0.8961576252779646\n",
            "Starting epoch 29\n",
            "0.3077357068928672 0.8954217734567583\n",
            "Starting epoch 30\n",
            "0.30272501207288854 0.8976799658685423\n",
            "Starting epoch 31\n",
            "0.3020909010301978 0.8951427315509128\n",
            "Starting epoch 32\n",
            "0.2980066799225131 0.8957600713657754\n",
            "Starting epoch 33\n",
            "0.3015537122842497 0.8963397028149833\n",
            "Starting epoch 34\n",
            "0.29746423255892956 0.897912680353726\n",
            "Starting epoch 35\n",
            "0.2917864499513769 0.8991990743134922\n",
            "Starting epoch 36\n",
            "0.2890293882831915 0.8953388150523176\n",
            "Starting epoch 37\n",
            "0.2873806931306588 0.8933294978538553\n",
            "Starting epoch 38\n",
            "0.2842652930614535 0.8961791729154815\n",
            "Starting epoch 39\n",
            "0.27883764399441263 0.8939005102480565\n",
            "Starting epoch 40\n",
            "0.2779043215658401 0.894526469117926\n",
            "Starting epoch 41\n",
            "0.2734261850021676 0.8934964920446121\n",
            "Starting epoch 42\n",
            "0.2707675887464161 0.8961296133491925\n",
            "Starting epoch 43\n",
            "0.26760656989089987 0.8974569478202411\n",
            "Starting epoch 44\n",
            "0.2641675956743148 0.8906457396011103\n",
            "Starting epoch 45\n",
            "0.2666210138183396 0.8930913964592921\n",
            "Starting epoch 46\n",
            "0.2559759149212515 0.8929922773267139\n",
            "Starting epoch 47\n",
            "0.25387057937438495 0.8918211632276637\n",
            "Starting epoch 48\n",
            "0.2545769687483478 0.8906845253486408\n",
            "Starting epoch 49\n",
            "0.24894565046835943 0.8916132285256244\n",
            "Starting epoch 50\n",
            "0.24703996775825937 0.8916983416938167\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs[:,None,:].float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val)[:,None,:]).detach().numpy().ravel()\n",
        "\n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qn3NQK7sIroe"
      },
      "source": [
        "The result is okay, but we don't think CNN is suitable for this kind of problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUn3MnoLJDdb"
      },
      "source": [
        "# feature selection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TS8RegBeLcUq"
      },
      "source": [
        "We borrow the indices from feature selection in tree-based."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "NL_Ps5juJrcK"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(Xtr)\n",
        "\n",
        "# standardize the training data\n",
        "Xtr_standardized = scaler.transform(Xtr)[:, [2,3,5,6,7]] # revise this line as needed\n",
        "Xts_standardized = scaler.transform(Xts)[:, [2,3,5,6,7]] # revise this line as needed\n",
        "ytr_standardized = ytr # revise this line as needed\n",
        "\n",
        "\n",
        "# save the standardized training data\n",
        "Xtr_savepath = 'Xtr_pytorch2.csv'\n",
        "Xts_savepath = 'Xts_pytorch2.csv'\n",
        "ytr_savepath = 'ytr_pytorch2.csv'\n",
        "yts_hat_savepath = 'yts_hat_pytorch2.csv'\n",
        "\n",
        "np.savetxt(Xtr_savepath, Xtr_standardized, delimiter=\",\")\n",
        "np.savetxt(Xts_savepath, Xts_standardized, delimiter=\",\")\n",
        "np.savetxt(ytr_savepath, ytr_standardized, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "H700xF9tJrcQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "sOLub1i5JrcQ"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(Xtr_standardized, ytr_standardized, test_size=0.2, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QkBbiKRuJrcQ"
      },
      "outputs": [],
      "source": [
        "train_data = []\n",
        "for i in range(len(X_train)):\n",
        "   train_data.append([X_train[i], y_train[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NAWGjnvQJrcQ"
      },
      "outputs": [],
      "source": [
        "all_data = []\n",
        "for i in range(len(Xtr_standardized)):\n",
        "   all_data.append([Xtr_standardized[i], ytr_standardized[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1rBwZLppKedq"
      },
      "outputs": [],
      "source": [
        "d_in = Xtr_standardized.shape[1]\n",
        "d_out = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "I_pPAMBiKuWM"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "UzeTMk5lK1zI"
      },
      "outputs": [],
      "source": [
        "class MLPD(nn.Module):\n",
        "    def __init__(self,n_layer,n_hidden_nodes):\n",
        "        super(MLPD, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in,n_hidden_nodes))\n",
        "        self.layers.append(nn.BatchNorm1d(n_hidden_nodes))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Dropout(0.2))\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Linear(n_hidden_nodes,n_hidden_nodes))\n",
        "            self.layers.append(nn.BatchNorm1d(n_hidden_nodes))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(0.2))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes,d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "L59gWtxpLECK"
      },
      "outputs": [],
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "s6ZAdH3EVGmC"
      },
      "outputs": [],
      "source": [
        "Allloader = torch.utils.data.DataLoader(all_data, batch_size=32, shuffle=True, num_workers=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu5cyJqHLfmW"
      },
      "source": [
        "First, we try some deep MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWTTC9GvK9jv",
        "outputId": "3a972644-b0ed-4d43-82b5-820b9552c5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.5780865664373851 0.7231678043819275\n",
            "Starting epoch 2\n",
            "0.5038453072265838 0.8215618589577839\n",
            "Starting epoch 3\n",
            "0.4550855139081614 0.8426634603782039\n",
            "Starting epoch 4\n",
            "0.4422248202261253 0.8488325489993276\n",
            "Starting epoch 5\n",
            "0.4276325636049005 0.8563752995121616\n",
            "Starting epoch 6\n",
            "0.4140186107094305 0.8670069038630605\n",
            "Starting epoch 7\n",
            "0.4141069663598391 0.8808932788609056\n",
            "Starting epoch 8\n",
            "0.4036070927361798 0.8809579217734567\n",
            "Starting epoch 9\n",
            "0.39998756833635163 0.8824215450518006\n",
            "Starting epoch 10\n",
            "0.39322589032112226 0.8859946820430608\n",
            "Starting epoch 11\n",
            "0.3914490696573048 0.888793720156522\n",
            "Starting epoch 12\n",
            "0.3946975353832895 0.8934350812776887\n",
            "Starting epoch 13\n",
            "0.3920657977093906 0.8956943510713486\n",
            "Starting epoch 14\n",
            "0.3824196977902529 0.8920818896416197\n",
            "Starting epoch 15\n",
            "0.3862215701235218 0.8913525021116682\n",
            "Starting epoch 16\n",
            "0.3761429365703607 0.9016964455017151\n",
            "Starting epoch 17\n",
            "0.3825812993380823 0.8964970005688577\n",
            "Starting epoch 18\n",
            "0.37378357194250567 0.8941418437882471\n",
            "Starting epoch 19\n",
            "0.37890202839086123 0.9027824464325732\n",
            "Starting epoch 20\n",
            "0.37469672609479493 0.9009649032080123\n",
            "Starting epoch 21\n",
            "0.3793471748117736 0.8945964989398562\n",
            "Starting epoch 22\n",
            "0.3743292766259656 0.9019690231163056\n",
            "Starting epoch 23\n",
            "0.3792173511377905 0.903073339539053\n",
            "Starting epoch 24\n",
            "0.37427488492672273 0.9010780283049766\n",
            "Starting epoch 25\n",
            "0.37620040963712564 0.8992044612228715\n",
            "Starting epoch 26\n",
            "0.3742346973994836 0.9029031132026685\n",
            "Starting epoch 27\n",
            "0.3698639909856447 0.9032220182379205\n",
            "Starting epoch 28\n",
            "0.3679418514921281 0.8994414852355588\n",
            "Starting epoch 29\n",
            "0.3705557050804564 0.900750504214718\n",
            "Starting epoch 30\n",
            "0.3714300157329399 0.9096475237454966\n",
            "Starting epoch 31\n",
            "0.37089482081789765 0.8981109186188826\n",
            "Starting epoch 32\n",
            "0.3702716609444833 0.9034881315612556\n",
            "Starting epoch 33\n",
            "0.3589397083738168 0.8953506662529519\n",
            "Starting epoch 34\n",
            "0.36576826386718675 0.9088071658823327\n",
            "Starting epoch 35\n",
            "0.3643689592278497 0.9058432883418661\n",
            "Starting epoch 36\n",
            "0.3641011999990396 0.903710072227681\n",
            "Starting epoch 37\n",
            "0.36081577980737345 0.9054920618503387\n",
            "Starting epoch 38\n",
            "0.3617938624150847 0.9065478960886728\n",
            "Starting epoch 39\n",
            "0.3589302159311869 0.900975677026771\n",
            "Starting epoch 40\n",
            "0.359031215222396 0.9099330299425971\n",
            "Starting epoch 41\n",
            "0.35858273374525196 0.9065177293961492\n",
            "Starting epoch 42\n",
            "0.3615507214059744 0.9050266328799712\n",
            "Starting epoch 43\n",
            "0.36020719405068397 0.9046947992622089\n",
            "Starting epoch 44\n",
            "0.35621233647843475 0.9054952939959664\n",
            "Starting epoch 45\n",
            "0.3594333597405512 0.904745436210374\n",
            "Starting epoch 46\n",
            "0.3584535619542321 0.9024646187791971\n",
            "Starting epoch 47\n",
            "0.3615530683643992 0.905302442640189\n",
            "Starting epoch 48\n",
            "0.3525409894531367 0.906669640240644\n",
            "Starting epoch 49\n",
            "0.3553872050109744 0.9059790384582236\n",
            "Starting epoch 50\n",
            "0.35257569537106476 0.9046064539483891\n",
            "Starting epoch 51\n",
            "0.35498253342265346 0.9094966902828774\n",
            "Starting epoch 52\n",
            "0.35700232654520914 0.9070402596059368\n",
            "Starting epoch 53\n",
            "0.35764628445588276 0.9029580596783369\n",
            "Starting epoch 54\n",
            "0.35517017806999956 0.9053433831514712\n",
            "Starting epoch 55\n",
            "0.3572620290105533 0.9081790522487114\n",
            "Starting epoch 56\n",
            "0.35161734490864366 0.9071910930685559\n",
            "Starting epoch 57\n",
            "0.36028933001442054 0.907739480443364\n",
            "Starting epoch 58\n",
            "0.3522778728810235 0.9076963851683303\n",
            "Starting epoch 59\n",
            "0.3515383655896185 0.9064552412473497\n",
            "Starting epoch 60\n",
            "0.3475159709214968 0.9048973470548689\n",
            "Starting epoch 61\n",
            "0.3534348873369954 0.9087619158435468\n",
            "Starting epoch 62\n",
            "0.35121864824386373 0.905591180982917\n",
            "Starting epoch 63\n",
            "0.34706680978995885 0.9073300753305408\n",
            "Starting epoch 64\n",
            "0.34334668484818304 0.903840435434659\n",
            "Starting epoch 65\n",
            "0.3525338266393455 0.9088922790505249\n",
            "Starting epoch 66\n",
            "0.3509518992407775 0.9046667873334368\n",
            "Starting epoch 67\n",
            "0.34482684115861045 0.9058357466687353\n",
            "Starting epoch 68\n",
            "0.34680383860778785 0.9118927875747702\n",
            "Starting epoch 69\n",
            "0.34641449086159876 0.9064196876454464\n",
            "Starting epoch 70\n",
            "0.34252269287358356 0.9069594559652481\n",
            "Starting epoch 71\n",
            "0.341458552455445 0.9058379014324871\n",
            "Starting epoch 72\n",
            "0.3470355165718454 0.9058411335781146\n",
            "Starting epoch 73\n",
            "0.34345499447667954 0.9089773922187171\n",
            "Starting epoch 74\n",
            "0.347497966106702 0.906517729396149\n",
            "Starting epoch 75\n",
            "0.34577062581471 0.9072891348192584\n",
            "Starting epoch 76\n",
            "0.3452153810023997 0.9086757252934788\n",
            "Starting epoch 77\n",
            "0.33859946489578646 0.9118173708434607\n",
            "Starting epoch 78\n",
            "0.3497199192948292 0.9070176345865439\n",
            "Starting epoch 79\n",
            "0.3398977597622122 0.909087285170054\n",
            "Starting epoch 80\n",
            "0.34480946205663077 0.9076532898932961\n",
            "Starting epoch 81\n",
            "0.34644038103808633 0.9057861871024461\n",
            "Starting epoch 82\n",
            "0.3336771389360006 0.909513928392891\n",
            "Starting epoch 83\n",
            "0.3427148749933458 0.9122483235938011\n",
            "Starting epoch 84\n",
            "0.34040469033306797 0.9068592594507939\n",
            "Starting epoch 85\n",
            "0.34278940653481854 0.9120091448173622\n",
            "Starting epoch 86\n",
            "0.3456735518646666 0.9092144162314044\n",
            "Starting epoch 87\n",
            "0.3407482259571243 0.9048876506179863\n",
            "Starting epoch 88\n",
            "0.34425357043201077 0.9100429228939338\n",
            "Starting epoch 89\n",
            "0.3357086730151393 0.9077340935339849\n",
            "Starting epoch 90\n",
            "0.33730916450139453 0.9088815052317665\n",
            "Starting epoch 91\n",
            "0.3402305597661308 0.9070284084053024\n",
            "Starting epoch 92\n",
            "0.33721453054938744 0.9067342831531952\n",
            "Starting epoch 93\n",
            "0.3417218311776051 0.90844947509955\n",
            "Starting epoch 94\n",
            "0.3393223447869784 0.9074259623174915\n",
            "Starting epoch 95\n",
            "0.3362367963561471 0.91348731275103\n",
            "Starting epoch 96\n",
            "0.34240706196408255 0.9099890538001412\n",
            "Starting epoch 97\n",
            "0.33650283873049397 0.9085496716140042\n",
            "Starting epoch 98\n",
            "0.33791045967291417 0.906662098567513\n",
            "Starting epoch 99\n",
            "0.3371066696368046 0.9114284359862784\n",
            "Starting epoch 100\n",
            "0.33224818228529474 0.909318922273362\n",
            "Starting epoch 101\n",
            "0.33645990831987593 0.9055739428729035\n",
            "Starting epoch 102\n",
            "0.3373447827641257 0.9043317215700472\n",
            "Starting epoch 103\n",
            "0.33522299510241077 0.9092488924514317\n",
            "Starting epoch 104\n",
            "0.33393612571986386 0.9064854079398734\n",
            "Starting epoch 105\n",
            "0.33767550100498067 0.9106527210356657\n",
            "Starting epoch 106\n",
            "0.33166459728196795 0.9062042112702763\n",
            "Starting epoch 107\n",
            "0.3425718568754437 0.9059176276913\n",
            "Starting epoch 108\n",
            "0.33743192544487194 0.9083837548051232\n",
            "Starting epoch 109\n",
            "0.3344725745820901 0.9109199117408767\n",
            "Starting epoch 110\n",
            "0.3416681778125112 0.9112603644136457\n",
            "Starting epoch 111\n",
            "0.3409812984238915 0.9103844529485787\n",
            "Starting epoch 112\n",
            "0.3362209244044996 0.9059133181637965\n",
            "Starting epoch 113\n",
            "0.3345685222899201 0.9078881591422316\n",
            "Starting epoch 114\n",
            "0.33083602278590296 0.9062742410922067\n",
            "Starting epoch 115\n",
            "0.3340068061759348 0.9048176207960561\n",
            "Starting epoch 116\n",
            "0.3330662995853462 0.9115728051576426\n",
            "Starting epoch 117\n",
            "0.3286739562706236 0.9066437830756238\n",
            "Starting epoch 118\n",
            "0.3314876967631283 0.9088998207236558\n",
            "Starting epoch 119\n",
            "0.3337703271360129 0.9069422178552342\n",
            "Starting epoch 120\n",
            "0.3304844240008182 0.9098080536449984\n",
            "Starting epoch 121\n",
            "0.3377773594638877 0.9120285376911277\n",
            "Starting epoch 122\n",
            "0.33854972046727416 0.9072514264536036\n",
            "Starting epoch 123\n",
            "0.33229477929333967 0.9097024702211649\n",
            "Starting epoch 124\n",
            "0.32922555873532455 0.9055556273810139\n",
            "Starting epoch 125\n",
            "0.33446806192294365 0.9077674923721363\n",
            "Starting epoch 126\n",
            "0.33763841391523935 0.9033028218786091\n",
            "Starting epoch 127\n",
            "0.32818691172766706 0.9056719846236059\n",
            "Starting epoch 128\n",
            "0.3329079854207297 0.9052054782713624\n",
            "Starting epoch 129\n",
            "0.33498224392519327 0.9048154660323042\n",
            "Starting epoch 130\n",
            "0.32133768486018355 0.9075724862526073\n",
            "Starting epoch 131\n",
            "0.33307151142311375 0.9060285980245125\n",
            "Starting epoch 132\n",
            "0.3382575691922002 0.9077836531002741\n",
            "Starting epoch 133\n",
            "0.33117916546184584 0.9093577080208926\n",
            "Starting epoch 134\n",
            "0.3318163596473026 0.9042336798193445\n",
            "Starting epoch 135\n",
            "0.3284747611785624 0.9104070779679716\n",
            "Starting epoch 136\n",
            "0.33504956602635516 0.9056967644067505\n",
            "Starting epoch 137\n",
            "0.3282738466945243 0.9057527882642947\n",
            "Starting epoch 138\n",
            "0.33126228454141055 0.9066416283118719\n",
            "Starting epoch 139\n",
            "0.33008809508572257 0.9095009998103809\n",
            "Starting epoch 140\n",
            "0.3308551325407483 0.911736567202772\n",
            "Starting epoch 141\n",
            "0.32899435555251083 0.90818982606747\n",
            "Starting epoch 142\n",
            "0.3328373969059106 0.9122838771957044\n",
            "Starting epoch 143\n",
            "0.32602045250789585 0.9085388977952458\n",
            "Starting epoch 144\n",
            "0.33480919114704544 0.9097746548068469\n",
            "Starting epoch 145\n",
            "94\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 500\n",
        "model = MLPD(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val).to(device)).to(torch.device('cpu')).detach().numpy().ravel()\n",
        "    \n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    if auc > auc_max:\n",
        "        auc_max = auc\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 50:\n",
        "        print(epoch - 50)\n",
        "        break\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R6akx8_QtuX",
        "outputId": "9fb42ece-91c3-4b7f-88da-5ad9a7da839a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Starting epoch 2\n",
            "Starting epoch 3\n",
            "Starting epoch 4\n",
            "Starting epoch 5\n",
            "Starting epoch 6\n",
            "Starting epoch 7\n",
            "Starting epoch 8\n",
            "Starting epoch 9\n",
            "Starting epoch 10\n",
            "Starting epoch 11\n",
            "Starting epoch 12\n",
            "Starting epoch 13\n",
            "Starting epoch 14\n",
            "Starting epoch 15\n",
            "Starting epoch 16\n",
            "Starting epoch 17\n",
            "Starting epoch 18\n",
            "Starting epoch 19\n",
            "Starting epoch 20\n",
            "Starting epoch 21\n",
            "Starting epoch 22\n",
            "Starting epoch 23\n",
            "Starting epoch 24\n",
            "Starting epoch 25\n",
            "Starting epoch 26\n",
            "Starting epoch 27\n",
            "Starting epoch 28\n",
            "Starting epoch 29\n",
            "Starting epoch 30\n",
            "Starting epoch 31\n",
            "Starting epoch 32\n",
            "Starting epoch 33\n",
            "Starting epoch 34\n",
            "Starting epoch 35\n",
            "Starting epoch 36\n",
            "Starting epoch 37\n",
            "Starting epoch 38\n",
            "Starting epoch 39\n",
            "Starting epoch 40\n",
            "Starting epoch 41\n",
            "Starting epoch 42\n",
            "Starting epoch 43\n",
            "Starting epoch 44\n",
            "Starting epoch 45\n",
            "Starting epoch 46\n",
            "Starting epoch 47\n",
            "Starting epoch 48\n",
            "Starting epoch 49\n",
            "Starting epoch 50\n",
            "Starting epoch 51\n",
            "Starting epoch 52\n",
            "Starting epoch 53\n",
            "Starting epoch 54\n",
            "Starting epoch 55\n",
            "Starting epoch 56\n",
            "Starting epoch 57\n",
            "Starting epoch 58\n",
            "Starting epoch 59\n",
            "Starting epoch 60\n",
            "Starting epoch 61\n",
            "Starting epoch 62\n",
            "Starting epoch 63\n",
            "Starting epoch 64\n",
            "Starting epoch 65\n",
            "Starting epoch 66\n",
            "Starting epoch 67\n",
            "Starting epoch 68\n",
            "Starting epoch 69\n",
            "Starting epoch 70\n",
            "Starting epoch 71\n",
            "Starting epoch 72\n",
            "Starting epoch 73\n",
            "Starting epoch 74\n",
            "Starting epoch 75\n",
            "Starting epoch 76\n",
            "Starting epoch 77\n",
            "Starting epoch 78\n",
            "Starting epoch 79\n",
            "Starting epoch 80\n",
            "Starting epoch 81\n",
            "Starting epoch 82\n",
            "Starting epoch 83\n",
            "Starting epoch 84\n",
            "Starting epoch 85\n",
            "Starting epoch 86\n",
            "Starting epoch 87\n",
            "Starting epoch 88\n",
            "Starting epoch 89\n",
            "Starting epoch 90\n",
            "Starting epoch 91\n",
            "Starting epoch 92\n",
            "Starting epoch 93\n",
            "Starting epoch 94\n",
            "Starting epoch 95\n",
            "Starting epoch 96\n",
            "Starting epoch 97\n",
            "Starting epoch 98\n",
            "Starting epoch 99\n",
            "Starting epoch 100\n",
            "Starting epoch 101\n",
            "Starting epoch 102\n",
            "Starting epoch 103\n",
            "Starting epoch 104\n",
            "Starting epoch 105\n",
            "Starting epoch 106\n",
            "Starting epoch 107\n",
            "Starting epoch 108\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 108\n",
        "model = MLPD(15, 512)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    for i, data in enumerate(Allloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMsziLzqQtuX",
        "outputId": "141da49e-3ca5-4256-a313-e6976ef90c5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLPD(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=5, out_features=512, bias=True)\n",
              "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): Dropout(p=0.2, inplace=False)\n",
              "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): Dropout(p=0.2, inplace=False)\n",
              "    (12): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (13): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (14): ReLU()\n",
              "    (15): Dropout(p=0.2, inplace=False)\n",
              "    (16): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): ReLU()\n",
              "    (19): Dropout(p=0.2, inplace=False)\n",
              "    (20): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (21): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU()\n",
              "    (23): Dropout(p=0.2, inplace=False)\n",
              "    (24): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (25): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU()\n",
              "    (27): Dropout(p=0.2, inplace=False)\n",
              "    (28): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (29): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (30): ReLU()\n",
              "    (31): Dropout(p=0.2, inplace=False)\n",
              "    (32): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (33): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (34): ReLU()\n",
              "    (35): Dropout(p=0.2, inplace=False)\n",
              "    (36): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (37): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (38): ReLU()\n",
              "    (39): Dropout(p=0.2, inplace=False)\n",
              "    (40): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (41): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU()\n",
              "    (43): Dropout(p=0.2, inplace=False)\n",
              "    (44): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (45): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (46): ReLU()\n",
              "    (47): Dropout(p=0.2, inplace=False)\n",
              "    (48): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (49): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (50): ReLU()\n",
              "    (51): Dropout(p=0.2, inplace=False)\n",
              "    (52): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (53): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (54): ReLU()\n",
              "    (55): Dropout(p=0.2, inplace=False)\n",
              "    (56): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (57): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (58): ReLU()\n",
              "    (59): Dropout(p=0.2, inplace=False)\n",
              "    (60): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (61): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (62): ReLU()\n",
              "    (63): Dropout(p=0.2, inplace=False)\n",
              "    (64): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "cnqv5vwoQtuY"
      },
      "outputs": [],
      "source": [
        "model.to(torch.device('cpu'))\n",
        "# save the model: you must use the .pth format for pytorch models!\n",
        "model_savepath = 'MLP3.pth'\n",
        "\n",
        "# To save a PyTorch model, we first pass an input through the model, \n",
        "# and then save the \"trace\". \n",
        "# For this purpose, we can use any input. \n",
        "# We will create a random input with the proper dimension.\n",
        "x = torch.randn(d_in) # random input\n",
        "x = x[None,:] # add singleton batch index\n",
        "with torch.no_grad():\n",
        "    traced_cell = torch.jit.trace(model, (x))\n",
        "\n",
        "# Now we save the trace\n",
        "torch.jit.save(traced_cell, model_savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "N_RqmSw7QtuY"
      },
      "outputs": [],
      "source": [
        "yts_hat_savepath = 'MLP3.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9fI1iUaQtuY",
        "outputId": "18662611-d40c-4d6f-841a-d64a65e594de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training auc =  0.9304228300782624\n",
            "test label confidences saved in MLP3.csv\n"
          ]
        }
      ],
      "source": [
        "# generate kaggle submission file using the validation script\n",
        "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iTapynnqOehF"
      },
      "source": [
        "It looks good, we try CNN with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "YVqGCstROiWA"
      },
      "outputs": [],
      "source": [
        "class CNNA(nn.Module):\n",
        "    def __init__(self, n_layer, n_hidden_nodes, n_channels, kernel_size):\n",
        "        super(CNNA, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in, n_hidden_nodes))\n",
        "        self.layers.append(nn.Conv1d(1, n_channels, kernel_size))\n",
        "        self.layers.append(nn.BatchNorm1d(n_channels))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Dropout(0.2))\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Conv1d(n_channels, n_channels, kernel_size))\n",
        "            self.layers.append(nn.BatchNorm1d(n_channels))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(0.2))\n",
        "        self.layers.append(nn.Flatten())\n",
        "        self.layers.append(nn.Linear(n_channels * (n_hidden_nodes - (kernel_size - 1) * (n_layer + 1)), n_hidden_nodes))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes, d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UB7LC8YOq-2",
        "outputId": "7453c53d-f820-4380-d470-17a67d2734bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "0.6092122762159274 0.8143283170433193\n",
            "Starting epoch 2\n",
            "0.5099515029262208 0.8456758201030838\n",
            "Starting epoch 3\n",
            "0.4713648650897441 0.8475019823826516\n",
            "Starting epoch 4\n",
            "0.443837797770616 0.8550479650411129\n",
            "Starting epoch 5\n",
            "0.4178286838517422 0.874970910689352\n",
            "Starting epoch 6\n",
            "0.40903361242839076 0.877467204495699\n",
            "Starting epoch 7\n",
            "0.39983566650853275 0.8794840633672923\n",
            "Starting epoch 8\n",
            "0.3873676279312793 0.8802877902466774\n",
            "Starting epoch 9\n",
            "0.38504950340242694 0.8790035510506629\n",
            "Starting epoch 10\n",
            "0.37346288955375156 0.8860442416093499\n",
            "Starting epoch 11\n",
            "0.3792730956779335 0.8887226129527158\n",
            "Starting epoch 12\n",
            "0.36996710834620006 0.8886579700401647\n",
            "Starting epoch 13\n",
            "0.36965415245250066 0.8900025426212269\n",
            "Starting epoch 14\n",
            "0.36363060749366116 0.8906662098567514\n",
            "Starting epoch 15\n",
            "0.3638016209380183 0.8914117581148403\n",
            "Starting epoch 16\n",
            "0.36019916809863894 0.8916056868524935\n",
            "Starting epoch 17\n",
            "0.3609921459781564 0.8942915998689904\n",
            "Starting epoch 18\n",
            "0.35476768808760434 0.8947150109461999\n",
            "Starting epoch 19\n",
            "0.35218238583344214 0.8944079571115824\n",
            "Starting epoch 20\n",
            "0.35488184812388596 0.8883552257330506\n",
            "Starting epoch 21\n",
            "0.35009374881708566 0.9001396286911102\n",
            "Starting epoch 22\n",
            "0.3546859682015667 0.8981927996414473\n",
            "Starting epoch 23\n",
            "0.34466021911229155 0.9003184740825017\n",
            "Starting epoch 24\n",
            "0.347809842242221 0.8976164003378669\n",
            "Starting epoch 25\n",
            "0.3470995048762099 0.8983878057609763\n",
            "Starting epoch 26\n",
            "0.3441674991844845 0.9022566840771578\n",
            "Starting epoch 27\n",
            "0.34352705132235334 0.9036508162245092\n",
            "Starting epoch 28\n",
            "0.33846004550183545 0.900091146506697\n",
            "Starting epoch 29\n",
            "0.33728570945311664 0.903217708710417\n",
            "Starting epoch 30\n",
            "0.33979746588385756 0.898701323886849\n",
            "Starting epoch 31\n",
            "0.3374187928060271 0.897998870903794\n",
            "Starting epoch 32\n",
            "0.33536764629504456 0.9074475099550084\n",
            "Starting epoch 33\n",
            "0.33541836004509135 0.9002764561893435\n",
            "Starting epoch 34\n",
            "0.3350745500932782 0.9045967575115065\n",
            "Starting epoch 35\n",
            "0.33607927902051354 0.900084682215442\n",
            "Starting epoch 36\n",
            "0.33105644663681427 0.8989394252814122\n",
            "Starting epoch 37\n",
            "0.3346190714534099 0.9038005723052526\n",
            "Starting epoch 38\n",
            "0.333990074314477 0.9029322025133165\n",
            "Starting epoch 39\n",
            "0.3319767532905923 0.9044825550326663\n",
            "Starting epoch 40\n",
            "0.33107047673272 0.9059876575132303\n",
            "Starting epoch 41\n",
            "0.328253285704591 0.9032263277654238\n",
            "Starting epoch 42\n",
            "0.3270490875753084 0.9019151540225129\n",
            "Starting epoch 43\n",
            "0.32406277299674807 0.8990784075433969\n",
            "Starting epoch 44\n",
            "0.32609155580502425 0.9075681767251038\n",
            "Starting epoch 45\n",
            "0.3245174047622552 0.9048424005792005\n",
            "Starting epoch 46\n",
            "0.32158364979535287 0.9084376238989158\n",
            "Starting epoch 47\n",
            "0.3296999734243694 0.9028707917463928\n",
            "Starting epoch 48\n",
            "0.32286875098978657 0.9053013652583131\n",
            "Starting epoch 49\n",
            "0.3259509871481062 0.9022426781127717\n",
            "Starting epoch 50\n",
            "0.32281512761371245 0.9057323180086534\n",
            "Starting epoch 51\n",
            "0.32250612510053733 0.9079387960903966\n",
            "Starting epoch 52\n",
            "0.3217473213958474 0.9057549430280464\n",
            "Starting epoch 53\n",
            "0.321098691656715 0.9075746410163591\n",
            "Starting epoch 54\n",
            "0.31899652386843463 0.9067472117357054\n",
            "Starting epoch 55\n",
            "0.3135738742142902 0.9036174173863579\n",
            "Starting epoch 56\n",
            "0.3183034986087044 0.9074259623174916\n",
            "Starting epoch 57\n",
            "0.31622143016792936 0.9026122200961887\n",
            "Starting epoch 58\n",
            "0.3169956465096853 0.9083040285463102\n",
            "Starting epoch 59\n",
            "0.31558585914020515 0.9061309493027182\n",
            "Starting epoch 60\n",
            "0.31326178356007534 0.9083449690575925\n",
            "Starting epoch 61\n",
            "0.3147297282673367 0.9052496509282721\n",
            "Starting epoch 62\n",
            "0.31339264861975724 0.9072589681267346\n",
            "Starting epoch 63\n",
            "0.3153097369655579 0.9091433090275982\n",
            "Starting epoch 64\n",
            "0.3145310105982827 0.9056364310217028\n",
            "Starting epoch 65\n",
            "0.3130634529781439 0.9044523883401425\n",
            "Starting epoch 66\n",
            "0.31358669604416783 0.9048467101067039\n",
            "Starting epoch 67\n",
            "0.3151954100702745 0.9075627898157246\n",
            "Starting epoch 68\n",
            "0.3165641123056435 0.9074679802106498\n",
            "Starting epoch 69\n",
            "0.3119524213439377 0.9071232180103774\n",
            "Starting epoch 70\n",
            "0.3140773625674338 0.9090344934581371\n",
            "Starting epoch 71\n",
            "0.31227308140946086 0.904991079278068\n",
            "Starting epoch 72\n",
            "0.31327988996581724 0.9084516298633019\n",
            "Starting epoch 73\n",
            "0.3088747059094822 0.9083891417145024\n",
            "Starting epoch 74\n",
            "0.3108701448165673 0.9100817086414645\n",
            "Starting epoch 75\n",
            "0.3089236735922624 0.9084914929927083\n",
            "Starting epoch 76\n",
            "0.3049990195944722 0.9087123562772578\n",
            "Starting epoch 77\n",
            "0.30923784018221284 0.9110104118184482\n",
            "Starting epoch 78\n",
            "0.30433462425339514 0.9066836462050302\n",
            "Starting epoch 79\n",
            "0.30859691278258033 0.9098382203375223\n",
            "Starting epoch 80\n",
            "0.30745894945427676 0.9077890400096533\n",
            "Starting epoch 81\n",
            "0.30795081913023387 0.905656901277344\n",
            "Starting epoch 82\n",
            "0.30543190265907094 0.908421463170778\n",
            "Starting epoch 83\n",
            "0.30696970481686087 0.9045213407801969\n",
            "Starting epoch 84\n",
            "0.3015507582948024 0.908002361621072\n",
            "Starting epoch 85\n",
            "0.3043065105227924 0.9069766940752616\n",
            "Starting epoch 86\n",
            "0.30377596729204737 0.9085916895071624\n",
            "Starting epoch 87\n",
            "0.30484579091808817 0.9087565289341677\n",
            "Starting epoch 88\n",
            "0.3012019638621466 0.9074184206443605\n",
            "Starting epoch 89\n",
            "0.3017009481347246 0.9071803192497976\n",
            "Starting epoch 90\n",
            "0.30369167893325105 0.9029041905845444\n",
            "Starting epoch 91\n",
            "0.3032909740928571 0.9099901311820172\n",
            "Starting epoch 92\n",
            "0.3012872175665503 0.9052205616176243\n",
            "Starting epoch 93\n",
            "0.2993508069779005 0.9050481805174881\n",
            "Starting epoch 94\n",
            "0.30336571741064655 0.9083697488407372\n",
            "Starting epoch 95\n",
            "0.3023479335587801 0.9095042319560084\n",
            "Starting epoch 96\n",
            "0.30418060262859253 0.9101377324990088\n",
            "Starting epoch 97\n",
            "0.301082698167585 0.9080842426436365\n",
            "Starting epoch 98\n",
            "0.299778552964045 0.9090398803675165\n",
            "Starting epoch 99\n",
            "0.29877533885282537 0.9099351847063488\n",
            "Starting epoch 100\n",
            "0.30037877326285395 0.9075078433400562\n",
            "Starting epoch 101\n",
            "0.29583257757573367 0.9107550723138715\n",
            "Starting epoch 102\n",
            "0.2967351175878073 0.9081575046111945\n",
            "Starting epoch 103\n",
            "0.29839261212996054 0.9078235162296806\n",
            "Starting epoch 104\n",
            "0.29951334249305317 0.9070704262984606\n",
            "Starting epoch 105\n",
            "0.2980567225688848 0.9061848183965111\n",
            "Starting epoch 106\n",
            "0.2966807911269269 0.907617736291393\n",
            "Starting epoch 107\n",
            "0.3002593587837323 0.9078763079415972\n",
            "Starting epoch 108\n",
            "0.296337210101953 0.9072471169261003\n",
            "Starting epoch 109\n",
            "0.2947535744783264 0.9061234076295875\n",
            "Starting epoch 110\n",
            "0.2958035914217951 0.9070790453534674\n",
            "Starting epoch 111\n",
            "0.2975133777557535 0.906785997483236\n",
            "Starting epoch 112\n",
            "0.29962137733150834 0.9046549361328023\n",
            "Starting epoch 113\n",
            "0.2960883951651003 0.9055459309441314\n",
            "Starting epoch 114\n",
            "0.29341584568178375 0.9050761924462603\n",
            "Starting epoch 115\n",
            "0.293723179136544 0.90713291444726\n",
            "Starting epoch 116\n",
            "0.29362709145275223 0.9139958369964317\n",
            "Starting epoch 117\n",
            "0.29388691152427027 0.9103219647997793\n",
            "Starting epoch 118\n",
            "0.29246548203850364 0.9079097067797486\n",
            "Starting epoch 119\n",
            "0.2925142739339881 0.9046183051490236\n",
            "Starting epoch 120\n",
            "0.2940497849039571 0.9054715915946975\n",
            "Starting epoch 121\n",
            "0.28929382243769375 0.9023859699022599\n",
            "Starting epoch 122\n",
            "0.28804149209600144 0.9057991156849563\n",
            "Starting epoch 123\n",
            "0.28932581260659396 0.904643084932168\n",
            "Starting epoch 124\n",
            "0.28923476159244943 0.9055189963972351\n",
            "Starting epoch 125\n",
            "0.28972545678994505 0.9073559324955612\n",
            "Starting epoch 126\n",
            "0.29124206866241836 0.9036885245901639\n",
            "Starting epoch 127\n",
            "0.2910056236314545 0.9064864853217492\n",
            "Starting epoch 128\n",
            "0.2925570213024293 0.9062613125096964\n",
            "Starting epoch 129\n",
            "0.2871067721016572 0.9054123355915258\n",
            "Starting epoch 130\n",
            "0.2871144469247602 0.9061729671958767\n",
            "Starting epoch 131\n",
            "0.28741436010280325 0.9037520901208391\n",
            "Starting epoch 132\n",
            "0.2864458145393013 0.9020498267569945\n",
            "Starting epoch 133\n",
            "0.2878625665927357 0.9076532898932962\n",
            "Starting epoch 134\n",
            "0.2881457834248055 0.901755701504887\n",
            "Starting epoch 135\n",
            "0.28963235838385665 0.9052949009670579\n",
            "Starting epoch 136\n",
            "0.2874539417112186 0.9024818568892107\n",
            "Starting epoch 137\n",
            "0.2885592543541106 0.9040365189360637\n",
            "Starting epoch 138\n",
            "0.28315803248493454 0.9040192808260502\n",
            "Starting epoch 139\n",
            "0.2853948921643505 0.9029893037527366\n",
            "Starting epoch 140\n",
            "0.2850815018997832 0.9039223164572237\n",
            "Starting epoch 141\n",
            "0.285347736883018 0.9052313354363828\n",
            "Starting epoch 142\n",
            "0.2855880073372558 0.9056547465135922\n",
            "Starting epoch 143\n",
            "0.2849237693521268 0.9073893313337126\n",
            "Starting epoch 144\n",
            "0.2830765824648998 0.9025895950767958\n",
            "Starting epoch 145\n",
            "0.28407225647925 0.9059865801313544\n",
            "Starting epoch 146\n",
            "0.28478644362697525 0.9075520159969661\n",
            "Starting epoch 147\n",
            "0.2800974730881669 0.9062214493802898\n",
            "Starting epoch 148\n",
            "0.2838970599758281 0.9052949009670579\n",
            "Starting epoch 149\n",
            "0.28361083699085515 0.9066577890400096\n",
            "Starting epoch 150\n",
            "0.27970270235952976 0.9031056609953285\n",
            "Starting epoch 151\n",
            "0.28073349060000413 0.9070919739359777\n",
            "Starting epoch 152\n",
            "0.27935545016083607 0.9061406457396011\n",
            "Starting epoch 153\n",
            "0.28342050557843257 0.906112633810829\n",
            "Starting epoch 154\n",
            "0.27973881676883083 0.9045364241264587\n",
            "Starting epoch 155\n",
            "0.28070265392055976 0.9037165365189361\n",
            "Starting epoch 156\n",
            "0.2815576886801868 0.9058045025943355\n",
            "Starting epoch 157\n",
            "0.27976955107164936 0.9029020358207925\n",
            "Starting epoch 158\n",
            "0.27725216373098027 0.9067332057713193\n",
            "Starting epoch 159\n",
            "0.28026154453301866 0.9037014531726743\n",
            "Starting epoch 160\n",
            "0.27860225157238727 0.9053692403164917\n",
            "Starting epoch 161\n",
            "0.28163348878194056 0.9052916688214304\n",
            "Starting epoch 162\n",
            "0.27807607967710885 0.8989135681163917\n",
            "Starting epoch 163\n",
            "0.2778178509868552 0.9048434779610763\n",
            "Starting epoch 164\n",
            "0.2754772434029513 0.9032284825291755\n",
            "Starting epoch 165\n",
            "0.2808448788339557 0.9062623898915724\n",
            "Starting epoch 166\n",
            "115\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 500\n",
        "model = CNNA(15, 512, 64, 5)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    current_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs[:,None,:].float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_loss += loss.item()\n",
        "    with torch.no_grad():\n",
        "        predict = model(torch.Tensor(X_val[:,None,:]).to(device)).to(torch.device('cpu')).detach().numpy().ravel()\n",
        "    \n",
        "    auc = roc_auc_score(y_val,predict)\n",
        "    if auc > auc_max:\n",
        "        auc_max = auc\n",
        "        count = 0\n",
        "    else:\n",
        "        count += 1\n",
        "    if count == 50:\n",
        "        print(epoch - 50)\n",
        "        break\n",
        "    print(current_loss / len(trainloader), auc)\n",
        "    current_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "hAOmaaMTY2qK"
      },
      "outputs": [],
      "source": [
        "class CNNA(nn.Module):\n",
        "    def __init__(self, n_layer, n_hidden_nodes, n_channels, kernel_size):\n",
        "        super(CNNA, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "        self.layers.append(nn.Linear(d_in, n_hidden_nodes))\n",
        "        self.layers.append(nn.Conv1d(1, n_channels, kernel_size))\n",
        "        self.layers.append(nn.BatchNorm1d(n_channels))\n",
        "        self.layers.append(nn.ReLU())\n",
        "        self.layers.append(nn.Dropout(0.2))\n",
        "        for i in range(n_layer):\n",
        "            self.layers.append(nn.Conv1d(n_channels, n_channels, kernel_size))\n",
        "            self.layers.append(nn.BatchNorm1d(n_channels))\n",
        "            self.layers.append(nn.ReLU())\n",
        "            self.layers.append(nn.Dropout(0.2))\n",
        "        self.layers.append(nn.Flatten())\n",
        "        self.layers.append(nn.Linear(n_channels * (n_hidden_nodes - (kernel_size - 1) * (n_layer + 1)), n_hidden_nodes))\n",
        "        self.layers.append(nn.Linear(n_hidden_nodes, d_out))\n",
        "    def forward(self,x):\n",
        "        out = self.layers(torch.unsqueeze(x, 1))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRgPlqT_X8-C",
        "outputId": "d582b222-e643-4b87-91c9-4da90fac2f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Starting epoch 2\n",
            "Starting epoch 3\n",
            "Starting epoch 4\n",
            "Starting epoch 5\n",
            "Starting epoch 6\n",
            "Starting epoch 7\n",
            "Starting epoch 8\n",
            "Starting epoch 9\n",
            "Starting epoch 10\n",
            "Starting epoch 11\n",
            "Starting epoch 12\n",
            "Starting epoch 13\n",
            "Starting epoch 14\n",
            "Starting epoch 15\n",
            "Starting epoch 16\n",
            "Starting epoch 17\n",
            "Starting epoch 18\n",
            "Starting epoch 19\n",
            "Starting epoch 20\n",
            "Starting epoch 21\n",
            "Starting epoch 22\n",
            "Starting epoch 23\n",
            "Starting epoch 24\n",
            "Starting epoch 25\n",
            "Starting epoch 26\n",
            "Starting epoch 27\n",
            "Starting epoch 28\n",
            "Starting epoch 29\n",
            "Starting epoch 30\n",
            "Starting epoch 31\n",
            "Starting epoch 32\n",
            "Starting epoch 33\n",
            "Starting epoch 34\n",
            "Starting epoch 35\n",
            "Starting epoch 36\n",
            "Starting epoch 37\n",
            "Starting epoch 38\n",
            "Starting epoch 39\n",
            "Starting epoch 40\n",
            "Starting epoch 41\n",
            "Starting epoch 42\n",
            "Starting epoch 43\n",
            "Starting epoch 44\n",
            "Starting epoch 45\n",
            "Starting epoch 46\n",
            "Starting epoch 47\n",
            "Starting epoch 48\n",
            "Starting epoch 49\n",
            "Starting epoch 50\n",
            "Starting epoch 51\n",
            "Starting epoch 52\n",
            "Starting epoch 53\n",
            "Starting epoch 54\n",
            "Starting epoch 55\n",
            "Starting epoch 56\n",
            "Starting epoch 57\n",
            "Starting epoch 58\n",
            "Starting epoch 59\n",
            "Starting epoch 60\n",
            "Starting epoch 61\n",
            "Starting epoch 62\n",
            "Starting epoch 63\n",
            "Starting epoch 64\n",
            "Starting epoch 65\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 65\n",
        "model = CNNA(15, 512, 64, 5)\n",
        "device = torch.device('cuda:0')\n",
        "model.to(device)\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.2)\n",
        "count = 0\n",
        "auc_max = 0\n",
        "for epoch in range(n_epochs): \n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "    for i, data in enumerate(Allloader):\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())[:, 0]\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUYhh09DX8-H",
        "outputId": "548cefa3-eb12-42e4-e8cc-3ea4f2744226"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNNA(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=5, out_features=512, bias=True)\n",
              "    (1): Conv1d(1, 64, kernel_size=(5,), stride=(1,))\n",
              "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU()\n",
              "    (4): Dropout(p=0.2, inplace=False)\n",
              "    (5): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU()\n",
              "    (8): Dropout(p=0.2, inplace=False)\n",
              "    (9): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU()\n",
              "    (12): Dropout(p=0.2, inplace=False)\n",
              "    (13): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (14): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (15): ReLU()\n",
              "    (16): Dropout(p=0.2, inplace=False)\n",
              "    (17): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (18): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU()\n",
              "    (20): Dropout(p=0.2, inplace=False)\n",
              "    (21): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (22): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (23): ReLU()\n",
              "    (24): Dropout(p=0.2, inplace=False)\n",
              "    (25): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (26): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (27): ReLU()\n",
              "    (28): Dropout(p=0.2, inplace=False)\n",
              "    (29): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (30): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (31): ReLU()\n",
              "    (32): Dropout(p=0.2, inplace=False)\n",
              "    (33): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (34): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (35): ReLU()\n",
              "    (36): Dropout(p=0.2, inplace=False)\n",
              "    (37): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (38): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (39): ReLU()\n",
              "    (40): Dropout(p=0.2, inplace=False)\n",
              "    (41): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (42): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (43): ReLU()\n",
              "    (44): Dropout(p=0.2, inplace=False)\n",
              "    (45): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (46): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (47): ReLU()\n",
              "    (48): Dropout(p=0.2, inplace=False)\n",
              "    (49): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (50): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (51): ReLU()\n",
              "    (52): Dropout(p=0.2, inplace=False)\n",
              "    (53): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (54): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (55): ReLU()\n",
              "    (56): Dropout(p=0.2, inplace=False)\n",
              "    (57): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (58): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (59): ReLU()\n",
              "    (60): Dropout(p=0.2, inplace=False)\n",
              "    (61): Conv1d(64, 64, kernel_size=(5,), stride=(1,))\n",
              "    (62): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (63): ReLU()\n",
              "    (64): Dropout(p=0.2, inplace=False)\n",
              "    (65): Flatten(start_dim=1, end_dim=-1)\n",
              "    (66): Linear(in_features=28672, out_features=512, bias=True)\n",
              "    (67): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztaG3T1PX8-H",
        "outputId": "0ec3b605-11f5-41f2-94b4-39098fa0d4be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 5])\n"
          ]
        }
      ],
      "source": [
        "model.to(torch.device('cpu'))\n",
        "# save the model: you must use the .pth format for pytorch models!\n",
        "model_savepath = 'MLP4.pth'\n",
        "\n",
        "# To save a PyTorch model, we first pass an input through the model, \n",
        "# and then save the \"trace\". \n",
        "# For this purpose, we can use any input. \n",
        "# We will create a random input with the proper dimension.\n",
        "x = torch.randn(2, d_in) # random input\n",
        "x = x # add singleton batch index\n",
        "print(x.shape)\n",
        "with torch.no_grad():\n",
        "    traced_cell = torch.jit.trace(model, (x))\n",
        "\n",
        "# Now we save the trace\n",
        "torch.jit.save(traced_cell, model_savepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ybKgf4zuX8-I"
      },
      "outputs": [],
      "source": [
        "yts_hat_savepath = 'MLP4.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wieNBb-9X8-I",
        "outputId": "5b3e4d0a-c302-4ec0-9e92-dda6c2bc7614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training auc =  0.9351776030566457\n",
            "test label confidences saved in MLP4.csv\n"
          ]
        }
      ],
      "source": [
        "# generate kaggle submission file using the validation script\n",
        "!python {\"validation.py \" + model_savepath + \" --Xts_path \" + Xts_savepath + \" --Xtr_path \" + Xtr_savepath + \" --yts_hat_path \" + yts_hat_savepath } "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "b5461e63495debc364e03189a3c6fbb263e94c99def73247209f067d5bf0cbac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
